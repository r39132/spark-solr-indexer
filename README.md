# ğŸš€ Spark Solr Indexer

A production-ready data pipeline that generates synthetic data and indexes it into Apache Solr using Apache Spark. Supports both local development and cloud deployment on Google Cloud Platform.

## âœ¨ Features

- ğŸ² **Synthetic Data Generation** â€“ Create realistic test data with Faker
- âš¡ **Spark-powered Indexing** â€“ Fast, distributed data processing
- ğŸ  **Local Development** â€“ Quick setup with local Solr and Spark
- â˜ï¸ **Cloud Deployment** â€“ Full GCP integration with Dataproc and Compute Engine
- ğŸ“Š **Interactive Notebooks** â€“ Step-by-step Jupyter workflows
- ğŸ› ï¸ **Make Automation** â€“ One-command pipeline execution

## ğŸ“‹ Prerequisites

### For Local Development
- **Python 3.12+**
- **uv** â€“ [Install](https://github.com/astral-sh/uv)
- **Java 11 or 17** â€“ For Solr and Spark
- **Apache Spark** â€“ In your PATH
- **jenv** (optional) â€“ [Install](https://www.jenv.be/)

### For Cloud Deployment
- **Google Cloud SDK** â€“ `gcloud` CLI
- **GCP Project** with billing enabled
- **APIs enabled**: Dataproc, Compute Engine, Cloud Storage

## ğŸš€ Quick Start

### Local Pipeline (Make)

Run the complete pipeline in one command:

```bash
make all
```

This will:
1. Generate synthetic data
2. Download and start Solr
3. Index data with Spark
4. Verify indexing

**Other useful commands:**
```bash
make help              # See all available commands
make verify-indexing   # Check if data was indexed
make stop-solr         # Stop Solr server
make clean-all         # Reset everything
```

### Interactive Notebooks

Choose your deployment target:

#### ğŸ  Local Development (`pipeline_local.ipynb`)

Perfect for local testing without cloud costs.

```bash
uv run jupyter notebook notebooks/pipeline_local.ipynb
```

**What it does:**
- âœ“ Verifies Java/Spark environment
- âœ“ Generates synthetic data
- âœ“ Starts local Solr (port 8983)
- âœ“ Indexes data with local Spark
- âœ“ Runs verification queries

**Requirements:** Java 11/17, Spark in PATH, Python 3.12+

#### â˜ï¸ GCP Cloud (`pipeline_gcp.ipynb`)

Production-scale deployment on Google Cloud Platform.

**Setup:**
```bash
cp .env.example .env      # Copy template
# Edit .env with your GCP settings
uv run jupyter notebook notebooks/pipeline_gcp.ipynb
```

**What it does:**
- âœ“ Authenticates with GCP
- âœ“ Creates Cloud Storage bucket
- âœ“ Provisions Solr VM (Compute Engine)
- âœ“ Creates Dataproc cluster
- âœ“ Uploads and indexes data
- âœ“ Verifies results
- âš ï¸ **Cleanup resources to avoid charges!**

**Costs:** ~$0.60-1.20/hour when running (Dataproc + VM + Storage)

**Configuration:** Edit `.env` file with your GCP project ID, region, and preferences. The notebook loads settings automatically.



## ğŸ§¹ Code Quality

This project uses modern Python tooling for code quality:

- **[Ruff](https://github.com/astral-sh/ruff)** â€“ Ultra-fast linter and formatter
- **[mypy](https://mypy-lang.org/)** â€“ Static type checker
- **[pre-commit](https://pre-commit.com/)** â€“ Git hooks for automated checks

### Setup

Install dev dependencies and pre-commit hooks:

```bash
uv sync --dev              # Install all dependencies including dev tools
uv run pre-commit install  # Set up git hooks
```

### Usage

```bash
# Format code
uv run ruff format .

# Lint and auto-fix
uv run ruff check --fix .

# Type check
uv run mypy data_gen/ spark_job/

# Run all pre-commit hooks manually
uv run pre-commit run --all-files
```

Pre-commit hooks will automatically run on `git commit` to ensure code quality.

## ğŸ› ï¸ Make Commands

### Pipeline Commands

| Target | Description |
|--------|-------------|
| `make all` | Run full pipeline (gen-data â†’ setup-solr â†’ index) |
| `make gen-data` | Generate dummy JSON data |
| `make setup-solr` | Download and start Solr |
| `make index` | Run Spark indexing job |
| `make verify-indexing-worked` | Verify documents in Solr |
| `make stop-solr` | Stop Solr server |
| `make restart-solr` | Restart Solr |
| `make check-env` | Verify Java 17 and Python 3.8+ |
| `make clean` | Remove generated data |
| `make clean-all` | Remove data, Solr, and Ivy cache |

### Code Quality Commands

| Target | Description |
|--------|-------------|
| `make format` | Format code with Ruff |
| `make lint` | Lint code with Ruff |
| `make lint-fix` | Lint and auto-fix issues |
| `make typecheck` | Type check with mypy |
| `make qa` | Run all quality checks (lint + typecheck) |
| `make precommit` | Run all pre-commit hooks |

## âš™ï¸ Configuration

### Environment Variables (.env)

For GCP deployments, configure your settings in `.env`:

```bash
cp .env.example .env    # Copy template
# Edit .env with your settings
```

**Key variables:**
- `GCP_PROJECT_ID` â€“ Your GCP project
- `GCP_REGION` â€“ Deployment region (e.g., us-central1)
- `GCS_BUCKET_NAME` â€“ Cloud Storage bucket
- `DATAPROC_WORKER_COUNT` â€“ Number of Spark workers
- `SOLR_VM_NAME` â€“ Solr VM instance name

ğŸ’¡ See `.env.example` for all available options.

âš ï¸ **Never commit `.env`** â€“ it's gitignored and contains your credentials.

## ğŸ“ Project Structure

```
spark-solr-indexer/
â”œâ”€â”€ data_gen/          # Synthetic data generation
â”œâ”€â”€ spark_job/         # PySpark indexing jobs
â”œâ”€â”€ scripts/           # Solr management scripts
â”œâ”€â”€ notebooks/         # Jupyter workflows
â”‚   â”œâ”€â”€ pipeline_local.ipynb   # Local development
â”‚   â””â”€â”€ pipeline_gcp.ipynb     # Cloud deployment
â”œâ”€â”€ .env.example       # GCP configuration template
â”œâ”€â”€ Makefile          # Task automation
â””â”€â”€ pyproject.toml    # Python dependencies
```

## âš¡ Performance

### Dependency Caching

Spark downloads ~240 dependencies from Maven Central on first run.

**First run:** 2-5 minutes (downloads and caches JARs to `~/.ivy2/`)

**Subsequent runs:** Near-instant (uses cached JARs)

**Optional pre-caching:**
```bash
./scripts/cache_dependencies.sh  # Download dependencies ahead of time
```

## License

MIT
