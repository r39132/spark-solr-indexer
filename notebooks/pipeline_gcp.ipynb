{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b21f644",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Google Cloud SDK (`gcloud`) installed\n",
    "- GCP project with billing enabled\n",
    "- Dataproc API enabled\n",
    "- Compute Engine API enabled (for Solr VM)\n",
    "- Appropriate IAM permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3427ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/r39132/Projects/spark-solr-indexer\n",
      "✓ Loaded configuration from .env file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "from google.cloud import storage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure we are in the project root\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "print(\"✓ Loaded configuration from .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a4c37",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Load GCP project details from `.env` file. Edit `.env` in the project root to customize settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329ea799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: family-tree-469815\n",
      "Region: us-central1\n",
      "Zone: us-central1-a\n",
      "Bucket: family-tree-469815-spark-solr-data\n",
      "Dataproc Cluster: spark-solr-cluster (2 workers)\n",
      "Solr VM: solr-instance\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from environment variables\n",
    "PROJECT_ID = os.getenv(\"GCP_PROJECT_ID\", \"your-project-id\")\n",
    "REGION = os.getenv(\"GCP_REGION\", \"us-central1\")\n",
    "ZONE = os.getenv(\"GCP_ZONE\", \"us-central1-a\")\n",
    "BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\", f\"{PROJECT_ID}-spark-solr-data\")\n",
    "\n",
    "# Dataproc Configuration\n",
    "CLUSTER_NAME = os.getenv(\"DATAPROC_CLUSTER_NAME\", \"spark-solr-cluster\")\n",
    "DATAPROC_MASTER_TYPE = os.getenv(\"DATAPROC_MASTER_TYPE\", \"n1-standard-4\")\n",
    "DATAPROC_WORKER_TYPE = os.getenv(\"DATAPROC_WORKER_TYPE\", \"n1-standard-4\")\n",
    "DATAPROC_WORKER_COUNT = int(os.getenv(\"DATAPROC_WORKER_COUNT\", \"2\"))\n",
    "\n",
    "# Solr Configuration (VM-based)\n",
    "SOLR_VM_NAME = os.getenv(\"SOLR_VM_NAME\", \"solr-instance\")\n",
    "SOLR_VM_TYPE = os.getenv(\"SOLR_VM_TYPE\", \"n1-standard-2\")\n",
    "SOLR_EXTERNAL_IP = None  # Will be set after VM creation\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Zone: {ZONE}\")\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Dataproc Cluster: {CLUSTER_NAME} ({DATAPROC_WORKER_COUNT} workers)\")\n",
    "print(f\"Solr VM: {SOLR_VM_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632356b",
   "metadata": {},
   "source": [
    "## 1. Authenticate with GCP\n",
    "\n",
    "Login to Google Cloud Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501dd782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8086%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=XTfpOXptqd3YkkDjgkYR2JvI9cw2qA&access_type=offline&code_challenge=zawIY4gOF1sd036pQsV9RT_LC5geQQ596vtobttW_Aw&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [r39132@gmail.com].\n",
      "Your current project is [family-tree-469815].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb59ba",
   "metadata": {},
   "source": [
    "## 2. Create GCS Bucket\n",
    "\n",
    "Create a Cloud Storage bucket to store data and scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2261b51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using existing bucket: family-tree-469815-spark-solr-data\n"
     ]
    }
   ],
   "source": [
    "# Check if bucket exists using gsutil (uses your authenticated credentials)\n",
    "check_result = subprocess.run(\n",
    "    f\"gsutil ls -b gs://{BUCKET_NAME}/\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if check_result.returncode == 0:\n",
    "    print(f\"✓ Using existing bucket: {BUCKET_NAME}\")\n",
    "else:\n",
    "    # Bucket doesn't exist, create it\n",
    "    print(f\"Creating bucket: {BUCKET_NAME}...\")\n",
    "    create_result = subprocess.run(\n",
    "        f\"gsutil mb -p {PROJECT_ID} -c STANDARD -l {REGION} gs://{BUCKET_NAME}/\",\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if create_result.returncode == 0:\n",
    "        print(f\"✓ Created bucket: {BUCKET_NAME}\")\n",
    "    else:\n",
    "        print(f\"✗ Failed to create bucket: {create_result.stderr}\")\n",
    "        raise Exception(f\"Bucket creation failed: {create_result.stderr}\")\n",
    "\n",
    "# Initialize storage client for Python API access (for blob operations)\n",
    "# storage_client = storage.Client(project=PROJECT_ID)\n",
    "# bucket = storage_client.bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3210c20",
   "metadata": {},
   "source": [
    "## 3. Generate and Upload Data\n",
    "\n",
    "Generate dummy data locally and upload to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db368df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Using existing local data: 1000 records\n",
      "⏭️  Data already exists in GCS: gs://family-tree-469815-spark-solr-data/data/dummy_data.json\n"
     ]
    }
   ],
   "source": [
    "# Check if data already exists locally\n",
    "local_data_exists = False\n",
    "if os.path.exists(\"data/dummy_data.json\"):\n",
    "    with open(\"data/dummy_data.json\") as f:\n",
    "        lines = f.readlines()\n",
    "    if len(lines) > 0:\n",
    "        print(f\"⏭️  Using existing local data: {len(lines)} records\")\n",
    "        local_data_exists = True\n",
    "\n",
    "# Generate data if needed\n",
    "if not local_data_exists:\n",
    "    print(\"Generating data...\")\n",
    "    !python3 data_gen/generate_data.py\n",
    "    with open(\"data/dummy_data.json\") as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"✓ Generated {len(lines)} records locally\")\n",
    "\n",
    "# Check if data already exists in GCS using gsutil\n",
    "check_result = subprocess.run(\n",
    "    f\"gsutil ls gs://{BUCKET_NAME}/data/dummy_data.json\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if check_result.returncode == 0:\n",
    "    print(f\"⏭️  Data already exists in GCS: gs://{BUCKET_NAME}/data/dummy_data.json\")\n",
    "else:\n",
    "    # Upload to GCS using gsutil\n",
    "    print(f\"Uploading data to GCS...\")\n",
    "    upload_result = subprocess.run(\n",
    "        f\"gsutil cp data/dummy_data.json gs://{BUCKET_NAME}/data/\",\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if upload_result.returncode == 0:\n",
    "        print(f\"✓ Uploaded to gs://{BUCKET_NAME}/data/dummy_data.json\")\n",
    "    else:\n",
    "        print(f\"✗ Upload failed: {upload_result.stderr}\")\n",
    "        raise Exception(f\"Upload failed: {upload_result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19986ec9",
   "metadata": {},
   "source": [
    "## 4. Create Solr VM on GCE\n",
    "\n",
    "Launch a Compute Engine VM and install Solr Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c62db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Solr VM IP: 34.121.247.203\n",
      "  Solr URL: http://34.121.247.203:8983\n"
     ]
    }
   ],
   "source": [
    "# Set Solr IP from existing VM\n",
    "ip_result = subprocess.run(\n",
    "    f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\",\n",
    "    shell=True, capture_output=True, text=True\n",
    ")\n",
    "SOLR_EXTERNAL_IP = ip_result.stdout.strip()\n",
    "print(f\"✓ Solr VM IP: {SOLR_EXTERNAL_IP}\")\n",
    "print(f\"  Solr URL: http://{SOLR_EXTERNAL_IP}:8983\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94c77bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Solr VM 'solr-instance' already exists\n",
      "  Solr URL: http://34.121.247.203:8983\n"
     ]
    }
   ],
   "source": [
    "# Check if Solr VM already exists\n",
    "vm_exists_cmd = f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(name)' 2>/dev/null\"\n",
    "result = subprocess.run(vm_exists_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.stdout.strip() == SOLR_VM_NAME:\n",
    "    print(f\"⏭️  Solr VM '{SOLR_VM_NAME}' already exists\")\n",
    "    # Get existing IP\n",
    "    ip_result = subprocess.run(\n",
    "        f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\",\n",
    "        shell=True, capture_output=True, text=True\n",
    "    )\n",
    "    SOLR_EXTERNAL_IP = ip_result.stdout.strip()\n",
    "    print(f\"  Solr URL: http://{SOLR_EXTERNAL_IP}:8983\")\n",
    "else:\n",
    "    # Create Solr VM with startup script\n",
    "    startup_script = \"\"\"#!/bin/bash\n",
    "apt-get update\n",
    "apt-get install -y openjdk-11-jdk wget\n",
    "\n",
    "# Download and setup Solr\n",
    "cd /opt\n",
    "wget https://archive.apache.org/dist/lucene/solr/8.11.3/solr-8.11.3.tgz\n",
    "tar xzf solr-8.11.3.tgz\n",
    "cd solr-8.11.3\n",
    "\n",
    "# Start Solr in cloud mode\n",
    "bin/solr start -c -m 2g\n",
    "\n",
    "# Create collection\n",
    "bin/solr create -c dummy_data -s 1 -rf 1\n",
    "\n",
    "echo \"Solr started on port 8983\"\n",
    "\"\"\"\n",
    "\n",
    "    # Create VM\n",
    "    create_vm_cmd = f\"\"\"\n",
    "gcloud compute instances create {SOLR_VM_NAME} \\\\\n",
    "    --project={PROJECT_ID} \\\\\n",
    "    --zone={ZONE} \\\\\n",
    "    --machine-type={SOLR_VM_TYPE} \\\\\n",
    "    --image-family=debian-11 \\\\\n",
    "    --image-project=debian-cloud \\\\\n",
    "    --boot-disk-size=50GB \\\\\n",
    "    --tags=solr-server \\\\\n",
    "    --metadata=startup-script='{startup_script}'\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Creating Solr VM...\")\n",
    "    !{create_vm_cmd}\n",
    "\n",
    "    # Create firewall rule for Solr\n",
    "    !gcloud compute firewall-rules create allow-solr \\\n",
    "        --project={PROJECT_ID} \\\n",
    "        --allow=tcp:8983 \\\n",
    "        --target-tags=solr-server \\\n",
    "        --description=\"Allow Solr traffic\" \\\n",
    "        2>/dev/null || echo \"Firewall rule already exists\"\n",
    "\n",
    "    # Wait for VM to be ready\n",
    "    time.sleep(60)\n",
    "\n",
    "    # Get external IP\n",
    "    result = subprocess.run(\n",
    "        f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\",\n",
    "        shell=True, capture_output=True, text=True\n",
    "    )\n",
    "    SOLR_EXTERNAL_IP = result.stdout.strip()\n",
    "    print(f\"✓ Solr VM created with IP: {SOLR_EXTERNAL_IP}\")\n",
    "    print(f\"  Solr URL: http://{SOLR_EXTERNAL_IP}:8983\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1e60f",
   "metadata": {},
   "source": [
    "## 5. Create Dataproc Cluster\n",
    "\n",
    "Launch a Dataproc cluster for running Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce303216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Dataproc cluster 'spark-solr-cluster' already exists\n"
     ]
    }
   ],
   "source": [
    "# Check if Dataproc cluster already exists\n",
    "cluster_exists_cmd = f\"gcloud dataproc clusters describe {CLUSTER_NAME} --region={REGION} --format='get(clusterName)' 2>/dev/null\"\n",
    "result = subprocess.run(cluster_exists_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.stdout.strip() == CLUSTER_NAME:\n",
    "    print(f\"⏭️  Dataproc cluster '{CLUSTER_NAME}' already exists\")\n",
    "else:\n",
    "    # Create Dataproc cluster\n",
    "    create_cluster_cmd = f\"\"\"\n",
    "gcloud dataproc clusters create {CLUSTER_NAME} \\\\\n",
    "    --project={PROJECT_ID} \\\\\n",
    "    --region={REGION} \\\\\n",
    "    --master-machine-type={DATAPROC_MASTER_TYPE} \\\\\n",
    "    --worker-machine-type={DATAPROC_WORKER_TYPE} \\\\\n",
    "    --num-workers={DATAPROC_WORKER_COUNT} \\\\\n",
    "    --image-version=2.1-debian11 \\\\\n",
    "    --enable-component-gateway \\\\\n",
    "    --optional-components=JUPYTER \\\\\n",
    "    --max-idle=3600s\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Creating Dataproc cluster (this may take 3-5 minutes)...\")\n",
    "    !{create_cluster_cmd}\n",
    "    print(f\"✓ Dataproc cluster '{CLUSTER_NAME}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337bb53",
   "metadata": {},
   "source": [
    "## 6. Index Data with Dataproc\n",
    "\n",
    "Submit the Spark job to index data into GCP Solr. This step checks if indexing is already complete and skips if verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a386e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  JAR already exists at gs://family-tree-469815-spark-solr-data/jars/spark-solr-4.0.0-shaded.jar\n"
     ]
    }
   ],
   "source": [
    "# Pre-download spark-solr JAR to GCS for faster job execution (avoids Maven dependency resolution)\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Check if JAR exists using gsutil (avoids Python client permission issues)\n",
    "check_jar_cmd = f\"gsutil -q stat gs://{BUCKET_NAME}/jars/spark-solr-4.0.0-shaded.jar\"\n",
    "jar_exists = subprocess.run(check_jar_cmd, shell=True).returncode == 0\n",
    "\n",
    "if not jar_exists:\n",
    "    print(\"Downloading spark-solr JAR (one-time setup)...\")\n",
    "    \n",
    "    # Create local jars directory\n",
    "    os.makedirs(\"jars\", exist_ok=True)\n",
    "    local_jar_path = \"jars/spark-solr-4.0.0-shaded.jar\"\n",
    "    \n",
    "    # Download JAR if not already local\n",
    "    if not os.path.exists(local_jar_path):\n",
    "        jar_url = \"https://repo1.maven.org/maven2/com/lucidworks/spark/spark-solr/4.0.0/spark-solr-4.0.0-shaded.jar\"\n",
    "        print(f\"  Downloading from Maven Central...\")\n",
    "        urllib.request.urlretrieve(jar_url, local_jar_path)\n",
    "        print(f\"  ✓ Downloaded to {local_jar_path}\")\n",
    "    \n",
    "    # Upload to GCS using gsutil\n",
    "    upload_cmd = f\"gsutil cp {local_jar_path} gs://{BUCKET_NAME}/jars/\"\n",
    "    result = subprocess.run(upload_cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ Uploaded JAR to gs://{BUCKET_NAME}/jars/\")\n",
    "    else:\n",
    "        print(f\"✗ Upload failed: {result.stderr}\")\n",
    "else:\n",
    "    print(f\"⏭️  JAR already exists at gs://{BUCKET_NAME}/jars/spark-solr-4.0.0-shaded.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbecf5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Skipping indexing: 1000 documents already indexed and verified in GCP Solr\n"
     ]
    }
   ],
   "source": [
    "# Check if indexing is already complete\n",
    "import requests\n",
    "\n",
    "def check_gcp_indexing_complete():\n",
    "    \"\"\"Check if data is already indexed in GCP Solr\"\"\"\n",
    "    try:\n",
    "        # Get local document count\n",
    "        with open(\"data/dummy_data.json\") as f:\n",
    "            local_count = sum(1 for _ in f)\n",
    "        \n",
    "        # Get Solr document count\n",
    "        response = requests.get(\n",
    "            f\"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=*:*&rows=0\", \n",
    "            timeout=10\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            solr_count = response.json()['response']['numFound']\n",
    "            \n",
    "            if local_count == solr_count and solr_count > 0:\n",
    "                # Verify sample document exists\n",
    "                with open(\"data/dummy_data.json\") as f:\n",
    "                    first_doc = json.loads(f.readline())\n",
    "                    doc_id = first_doc['id']\n",
    "                \n",
    "                check_response = requests.get(\n",
    "                    f\"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=id:{doc_id}&rows=1\",\n",
    "                    timeout=10\n",
    "                )\n",
    "                if check_response.status_code == 200:\n",
    "                    match_count = check_response.json()['response']['numFound']\n",
    "                    if match_count > 0:\n",
    "                        return True, solr_count\n",
    "        return False, 0\n",
    "    except Exception as e:\n",
    "        print(f\"Check failed: {e}\")\n",
    "        return False, 0\n",
    "\n",
    "already_indexed, doc_count = check_gcp_indexing_complete()\n",
    "\n",
    "if already_indexed:\n",
    "    print(f\"⏭️  Skipping indexing: {doc_count} documents already indexed and verified in GCP Solr\")\n",
    "else:\n",
    "    # Get Solr Internal IP for ZooKeeper connection (required by spark-solr)\n",
    "    ip_result = subprocess.run(\n",
    "        f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(networkInterfaces[0].networkIP)'\",\n",
    "        shell=True, capture_output=True, text=True\n",
    "    )\n",
    "    SOLR_INTERNAL_IP = ip_result.stdout.strip()\n",
    "    print(f\"✓ Solr Internal IP: {SOLR_INTERNAL_IP}\")\n",
    "\n",
    "    # Always upload the latest version of the job to ensure fixes are applied\n",
    "    # Modify the Spark job to use GCS paths and remote Solr\n",
    "    gcp_spark_job = f\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\\\n",
    "        .appName(\"SolrIndexer-GCP\") \\\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Read JSON data from GCS\n",
    "    input_file = \"gs://{BUCKET_NAME}/data/dummy_data.json\"\n",
    "    print(f\"Reading data from {{input_file}}\")\n",
    "    \n",
    "    df = spark.read.json(input_file)\n",
    "    \n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Solr configuration\n",
    "    # Use Internal IP for ZooKeeper (accessible within VPC)\n",
    "    zk_host = \"{SOLR_INTERNAL_IP}:9983\"\n",
    "    collection = \"dummy_data\"\n",
    "    \n",
    "    print(f\"Indexing to Solr collection '{{collection}}' via ZK '{{zk_host}}'...\")\n",
    "    \n",
    "    # Write to Solr using ZK (standard method)\n",
    "    df.write.format(\"solr\") \\\\\n",
    "        .option(\"zkhost\", zk_host) \\\\\n",
    "        .option(\"collection\", collection) \\\\\n",
    "        .option(\"gen_uniq_key\", \"true\") \\\\\n",
    "        .option(\"commit_within\", \"1000\") \\\\\n",
    "        .mode(\"overwrite\") \\\\\n",
    "        .save()\n",
    "        \n",
    "    print(\"Indexing complete.\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "    # Save and upload job\n",
    "    os.makedirs(\"spark_job\", exist_ok=True)\n",
    "    with open(\"spark_job/index_to_solr_gcp.py\", \"w\") as f:\n",
    "        f.write(gcp_spark_job)\n",
    "    \n",
    "    # Upload using gsutil\n",
    "    upload_job_cmd = f\"gsutil cp spark_job/index_to_solr_gcp.py gs://{BUCKET_NAME}/jobs/\"\n",
    "    subprocess.run(upload_job_cmd, shell=True, check=True)\n",
    "    print(f\"✓ Uploaded job to gs://{BUCKET_NAME}/jobs/index_to_solr_gcp.py\")\n",
    "    \n",
    "    # Submit job to Dataproc using pre-downloaded JAR (much faster)\n",
    "    submit_job_cmd = f\"\"\"\n",
    "gcloud dataproc jobs submit pyspark \\\\\n",
    "    gs://{BUCKET_NAME}/jobs/index_to_solr_gcp.py \\\\\n",
    "    --cluster={CLUSTER_NAME} \\\\\n",
    "    --region={REGION} \\\\\n",
    "    --jars=gs://{BUCKET_NAME}/jars/spark-solr-4.0.0-shaded.jar\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Submitting Spark job to Dataproc...\")\n",
    "    !{submit_job_cmd}\n",
    "    print(\"✓ Job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad68bc2",
   "metadata": {},
   "source": [
    "## 7. Verify Indexing\n",
    "\n",
    "Query the GCP-hosted Solr instance to verify data was indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b85eef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Indexed 1000 documents in Solr\n",
      "\n",
      "Sample documents:\n",
      "{\n",
      "    \"responseHeader\": {\n",
      "        \"zkConnected\": true,\n",
      "        \"status\": 0,\n",
      "        \"QTime\": 1,\n",
      "        \"params\": {\n",
      "            \"q\": \"*:*\",\n",
      "            \"rows\": \"3\"\n",
      "        }\n",
      "    },\n",
      "    \"response\": {\n",
      "        \"numFound\": 1000,\n",
      "        \"start\": 0,\n",
      "        \"numFoundExact\": true,\n",
      "        \"docs\": [\n",
      "            {\n",
      "                \"author\": \"Laura Neal\",\n",
      "                \"category\": \"Music\",\n",
      "                \"created_at\": \"2025-11-12T03:06:57.210749\",\n",
      "                \"description\": \"Game small too thus quickly. Food before every care. Easy art simple cut. Raise concern from really.\",\n",
      "                \"id\": \"0\",\n",
      "                \"in_stock\": true,\n",
      "                \"price\": 38.33,\n",
      "                \"title\": \"Itself begin trip fly with you too.\",\n",
      "                \"_version_\": 1849728360208400384\n",
      "            },\n",
      "            {\n",
      "                \"author\": \"Julie Nichols\",\n",
      "                \"category\": \"Music\",\n",
      "                \"created_at\": \"2025-06-15T15:01:16.289494\",\n",
      "                \"description\": \"Left over alone live tree. Project letter ten soldier fine increase foreign.\",\n",
      "                \"id\": \"1\",\n",
      "                \"in_stock\": false,\n",
      "                \"price\": 320.13,\n",
      "                \"title\": \"Treatment result put fish deal.\",\n",
      "                \"_version_\": 1849728360293335040\n",
      "            },\n",
      "            {\n",
      "                \"author\": \"Kathleen Munoz\",\n",
      "                \"category\": \"History\",\n",
      "                \"created_at\": \"2025-08-01T08:21:03.893208\",\n",
      "                \"description\": \"During list likely buy. If affect business car.\",\n",
      "                \"id\": \"2\",\n",
      "                \"in_stock\": true,\n",
      "                \"price\": 45.13,\n",
      "                \"title\": \"Quickly expert threat health ball activity take.\",\n",
      "                \"_version_\": 1849728360295432192\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Query Solr for document count\n",
    "try:\n",
    "    response = requests.get(f\"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=*:*&rows=0\")\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        num_docs = result['response']['numFound']\n",
    "        print(f\"✓ Indexed {num_docs} documents in Solr\")\n",
    "    else:\n",
    "        print(\"✗ Failed to query Solr\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Query failed: {e}\")\n",
    "\n",
    "# Show sample documents\n",
    "print(\"\\nSample documents:\")\n",
    "!curl -s \"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=*:*&rows=3\" | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12985d18",
   "metadata": {},
   "source": [
    "## 8. Cleanup Resources\n",
    "\n",
    "**Important:** Delete GCP resources to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f74caeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To clean up, uncomment the commands above\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to clean up all GCP resources\n",
    "\n",
    "# # Delete Dataproc cluster\n",
    "# !gcloud dataproc clusters delete {CLUSTER_NAME} --region={REGION} --quiet\n",
    "# print(\"✓ Deleted Dataproc cluster\")\n",
    "\n",
    "# # Delete Solr VM\n",
    "# !gcloud compute instances delete {SOLR_VM_NAME} --zone={ZONE} --quiet\n",
    "# print(\"✓ Deleted Solr VM\")\n",
    "\n",
    "# # Delete firewall rule\n",
    "# !gcloud compute firewall-rules delete allow-solr --quiet\n",
    "# print(\"✓ Deleted firewall rule\")\n",
    "\n",
    "# # Delete GCS bucket\n",
    "# !gsutil -m rm -r gs://{BUCKET_NAME}\n",
    "# print(\"✓ Deleted GCS bucket\")\n",
    "\n",
    "print(\"To clean up, uncomment the commands above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3d444",
   "metadata": {},
   "source": [
    "## Cost Estimation\n",
    "\n",
    "- **Dataproc Cluster**: ~$0.50-1.00/hour (2 workers + 1 master)\n",
    "- **Solr VM**: ~$0.10-0.20/hour (n1-standard-2)\n",
    "- **Storage**: ~$0.02/GB/month\n",
    "- **Network Egress**: Variable\n",
    "\n",
    "**Remember to delete resources when not in use!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a998970-ac02-49c3-a42b-9fcd4f955a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
