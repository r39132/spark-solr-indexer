{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b21f644",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Google Cloud SDK (`gcloud`) installed\n",
    "- GCP project with billing enabled\n",
    "- Dataproc API enabled\n",
    "- Compute Engine API enabled (for Solr VM)\n",
    "- Appropriate IAM permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3427ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/r39132/Projects/spark-solr-indexer\n",
      "✓ Loaded configuration from .env file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "from google.cloud import storage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure we are in the project root\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "print(\"✓ Loaded configuration from .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a4c37",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Load GCP project details from `.env` file. Edit `.env` in the project root to customize settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329ea799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: family-tree-469815\n",
      "Region: us-central1\n",
      "Zone: us-central1-a\n",
      "Bucket: family-tree-469815-spark-solr-data\n",
      "Dataproc Cluster: spark-solr-cluster (2 workers)\n",
      "Solr VM: solr-instance\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from environment variables\n",
    "PROJECT_ID = os.getenv(\"GCP_PROJECT_ID\", \"your-project-id\")\n",
    "REGION = os.getenv(\"GCP_REGION\", \"us-central1\")\n",
    "ZONE = os.getenv(\"GCP_ZONE\", \"us-central1-a\")\n",
    "BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\", f\"{PROJECT_ID}-spark-solr-data\")\n",
    "\n",
    "# Dataproc Configuration\n",
    "CLUSTER_NAME = os.getenv(\"DATAPROC_CLUSTER_NAME\", \"spark-solr-cluster\")\n",
    "DATAPROC_MASTER_TYPE = os.getenv(\"DATAPROC_MASTER_TYPE\", \"n1-standard-4\")\n",
    "DATAPROC_WORKER_TYPE = os.getenv(\"DATAPROC_WORKER_TYPE\", \"n1-standard-4\")\n",
    "DATAPROC_WORKER_COUNT = int(os.getenv(\"DATAPROC_WORKER_COUNT\", \"2\"))\n",
    "\n",
    "# Solr Configuration (VM-based)\n",
    "SOLR_VM_NAME = os.getenv(\"SOLR_VM_NAME\", \"solr-instance\")\n",
    "SOLR_VM_TYPE = os.getenv(\"SOLR_VM_TYPE\", \"n1-standard-2\")\n",
    "SOLR_EXTERNAL_IP = None  # Will be set after VM creation\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Zone: {ZONE}\")\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Dataproc Cluster: {CLUSTER_NAME} ({DATAPROC_WORKER_COUNT} workers)\")\n",
    "print(f\"Solr VM: {SOLR_VM_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632356b",
   "metadata": {},
   "source": [
    "## 1. Authenticate with GCP\n",
    "\n",
    "Login to Google Cloud Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501dd782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=RnMD6UmwQgHtpEOtFd2ker4e7924Xj&access_type=offline&code_challenge=N7n-OopMkZ_MHl2B8aBfynei82SmBxg1hBGiGtVHKK8&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [r39132@gmail.com].\n",
      "Your current project is [family-tree-469815].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb59ba",
   "metadata": {},
   "source": [
    "## 2. Create GCS Bucket\n",
    "\n",
    "Create a Cloud Storage bucket to store data and scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2261b51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using existing bucket: family-tree-469815-spark-solr-data\n"
     ]
    }
   ],
   "source": [
    "# Check if bucket exists using gsutil (uses your authenticated credentials)\n",
    "check_result = subprocess.run(\n",
    "    f\"gsutil ls -b gs://{BUCKET_NAME}/\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if check_result.returncode == 0:\n",
    "    print(f\"✓ Using existing bucket: {BUCKET_NAME}\")\n",
    "else:\n",
    "    # Bucket doesn't exist, create it\n",
    "    print(f\"Creating bucket: {BUCKET_NAME}...\")\n",
    "    create_result = subprocess.run(\n",
    "        f\"gsutil mb -p {PROJECT_ID} -c STANDARD -l {REGION} gs://{BUCKET_NAME}/\",\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if create_result.returncode == 0:\n",
    "        print(f\"✓ Created bucket: {BUCKET_NAME}\")\n",
    "    else:\n",
    "        print(f\"✗ Failed to create bucket: {create_result.stderr}\")\n",
    "        raise Exception(f\"Bucket creation failed: {create_result.stderr}\")\n",
    "\n",
    "# Initialize storage client for Python API access (for blob operations)\n",
    "# storage_client = storage.Client(project=PROJECT_ID)\n",
    "# bucket = storage_client.bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3210c20",
   "metadata": {},
   "source": [
    "## 3. Generate and Upload Data\n",
    "\n",
    "Generate dummy data locally and upload to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db368df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Using existing local data: 1000 records\n",
      "⏭️  Data already exists in GCS: gs://family-tree-469815-spark-solr-data/data/dummy_data.json\n",
      "⏭️  Data already exists in GCS: gs://family-tree-469815-spark-solr-data/data/dummy_data.json\n"
     ]
    }
   ],
   "source": [
    "# Check if data already exists locally\n",
    "local_data_exists = False\n",
    "if os.path.exists(\"data/dummy_data.json\"):\n",
    "    with open(\"data/dummy_data.json\") as f:\n",
    "        lines = f.readlines()\n",
    "    if len(lines) > 0:\n",
    "        print(f\"⏭️  Using existing local data: {len(lines)} records\")\n",
    "        local_data_exists = True\n",
    "\n",
    "# Generate data if needed\n",
    "if not local_data_exists:\n",
    "    print(\"Generating data...\")\n",
    "    !python3 data_gen/generate_data.py\n",
    "    with open(\"data/dummy_data.json\") as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"✓ Generated {len(lines)} records locally\")\n",
    "\n",
    "# Check if data already exists in GCS using gsutil\n",
    "check_result = subprocess.run(\n",
    "    f\"gsutil ls gs://{BUCKET_NAME}/data/dummy_data.json\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if check_result.returncode == 0:\n",
    "    print(f\"⏭️  Data already exists in GCS: gs://{BUCKET_NAME}/data/dummy_data.json\")\n",
    "else:\n",
    "    # Upload to GCS using gsutil\n",
    "    print(f\"Uploading data to GCS...\")\n",
    "    upload_result = subprocess.run(\n",
    "        f\"gsutil cp data/dummy_data.json gs://{BUCKET_NAME}/data/\",\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if upload_result.returncode == 0:\n",
    "        print(f\"✓ Uploaded to gs://{BUCKET_NAME}/data/dummy_data.json\")\n",
    "    else:\n",
    "        print(f\"✗ Upload failed: {upload_result.stderr}\")\n",
    "        raise Exception(f\"Upload failed: {upload_result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19986ec9",
   "metadata": {},
   "source": [
    "## 4. Create Solr VM on GCE\n",
    "\n",
    "Launch a Compute Engine VM and install Solr Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c62db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Solr VM IP: 34.121.247.203\n",
      "  Solr URL: http://34.121.247.203:8983\n"
     ]
    }
   ],
   "source": [
    "# Set Solr IP from existing VM\n",
    "ip_result = subprocess.run(\n",
    "    f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\",\n",
    "    shell=True, capture_output=True, text=True\n",
    ")\n",
    "SOLR_EXTERNAL_IP = ip_result.stdout.strip()\n",
    "print(f\"✓ Solr VM IP: {SOLR_EXTERNAL_IP}\")\n",
    "print(f\"  Solr URL: http://{SOLR_EXTERNAL_IP}:8983\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c77bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Solr VM 'solr-instance' already exists\n",
      "  Solr URL: http://34.121.247.203:8983\n",
      "  Solr URL: http://34.121.247.203:8983\n"
     ]
    }
   ],
   "source": [
    "# Check if Solr VM already exists\n",
    "vm_exists_cmd = f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(name)' 2>/dev/null\"\n",
    "result = subprocess.run(vm_exists_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.stdout.strip() == SOLR_VM_NAME:\n",
    "    print(f\"⏭️  Solr VM '{SOLR_VM_NAME}' already exists\")\n",
    "    # Get existing IP\n",
    "    ip_result = subprocess.run(\n",
    "        f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\",\n",
    "        shell=True, capture_output=True, text=True\n",
    "    )\n",
    "    SOLR_EXTERNAL_IP = ip_result.stdout.strip()\n",
    "    print(f\"  Solr URL: http://{SOLR_EXTERNAL_IP}:8983\")\n",
    "else:\n",
    "    # Create Solr VM with startup script\n",
    "    startup_script = \"\"\"#!/bin/bash\n",
    "apt-get update\n",
    "apt-get install -y openjdk-11-jdk wget\n",
    "\n",
    "# Download and setup Solr\n",
    "cd /opt\n",
    "wget https://archive.apache.org/dist/lucene/solr/8.11.3/solr-8.11.3.tgz\n",
    "tar xzf solr-8.11.3.tgz\n",
    "cd solr-8.11.3\n",
    "\n",
    "# Start Solr in cloud mode\n",
    "bin/solr start -c -m 2g\n",
    "\n",
    "# Create collection\n",
    "bin/solr create -c dummy_data -s 1 -rf 1\n",
    "\n",
    "echo \"Solr started on port 8983\"\n",
    "\"\"\"\n",
    "\n",
    "    # Create VM\n",
    "    create_vm_cmd = f\"\"\"\n",
    "gcloud compute instances create {SOLR_VM_NAME} \\\\\n",
    "    --project={PROJECT_ID} \\\\\n",
    "    --zone={ZONE} \\\\\n",
    "    --machine-type={SOLR_VM_TYPE} \\\\\n",
    "    --image-family=debian-11 \\\\\n",
    "    --image-project=debian-cloud \\\\\n",
    "    --boot-disk-size=50GB \\\\\n",
    "    --tags=solr-server \\\\\n",
    "    --metadata=startup-script='{startup_script}'\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Creating Solr VM...\")\n",
    "    !{create_vm_cmd}\n",
    "\n",
    "    # Create firewall rule for Solr\n",
    "    !gcloud compute firewall-rules create allow-solr \\\n",
    "        --project={PROJECT_ID} \\\n",
    "        --allow=tcp:8983 \\\n",
    "        --target-tags=solr-server \\\n",
    "        --description=\"Allow Solr traffic\" \\\n",
    "        2>/dev/null || echo \"Firewall rule already exists\"\n",
    "\n",
    "    # Wait for VM to be ready\n",
    "    time.sleep(60)\n",
    "\n",
    "    # Get external IP\n",
    "    result = subprocess.run(\n",
    "        f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\",\n",
    "        shell=True, capture_output=True, text=True\n",
    "    )\n",
    "    SOLR_EXTERNAL_IP = result.stdout.strip()\n",
    "    print(f\"✓ Solr VM created with IP: {SOLR_EXTERNAL_IP}\")\n",
    "    print(f\"  Solr URL: http://{SOLR_EXTERNAL_IP}:8983\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1e60f",
   "metadata": {},
   "source": [
    "## 5. Create Dataproc Cluster\n",
    "\n",
    "Launch a Dataproc cluster for running Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce303216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Dataproc cluster 'spark-solr-cluster' already exists\n"
     ]
    }
   ],
   "source": [
    "# Check if Dataproc cluster already exists\n",
    "cluster_exists_cmd = f\"gcloud dataproc clusters describe {CLUSTER_NAME} --region={REGION} --format='get(clusterName)' 2>/dev/null\"\n",
    "result = subprocess.run(cluster_exists_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.stdout.strip() == CLUSTER_NAME:\n",
    "    print(f\"⏭️  Dataproc cluster '{CLUSTER_NAME}' already exists\")\n",
    "else:\n",
    "    # Create Dataproc cluster\n",
    "    create_cluster_cmd = f\"\"\"\n",
    "gcloud dataproc clusters create {CLUSTER_NAME} \\\\\n",
    "    --project={PROJECT_ID} \\\\\n",
    "    --region={REGION} \\\\\n",
    "    --master-machine-type={DATAPROC_MASTER_TYPE} \\\\\n",
    "    --worker-machine-type={DATAPROC_WORKER_TYPE} \\\\\n",
    "    --num-workers={DATAPROC_WORKER_COUNT} \\\\\n",
    "    --image-version=2.1-debian11 \\\\\n",
    "    --enable-component-gateway \\\\\n",
    "    --optional-components=JUPYTER \\\\\n",
    "    --max-idle=3600s\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Creating Dataproc cluster (this may take 3-5 minutes)...\")\n",
    "    !{create_cluster_cmd}\n",
    "    print(f\"✓ Dataproc cluster '{CLUSTER_NAME}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337bb53",
   "metadata": {},
   "source": [
    "## 6. Index Data with Dataproc\n",
    "\n",
    "Submit the Spark job to index data into GCP Solr. This step checks if indexing is already complete and skips if verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a386e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading spark-solr JAR (one-time setup)...\n",
      "  Downloading from Maven Central...\n",
      "  ✓ Downloaded to jars/spark-solr-4.0.0-shaded.jar\n",
      "  ✓ Downloaded to jars/spark-solr-4.0.0-shaded.jar\n",
      "✓ Uploaded JAR to gs://family-tree-469815-spark-solr-data/jars/\n",
      "✓ Uploaded JAR to gs://family-tree-469815-spark-solr-data/jars/\n"
     ]
    }
   ],
   "source": [
    "# Pre-download spark-solr JAR to GCS for faster job execution (avoids Maven dependency resolution)\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Check if JAR exists using gsutil (avoids Python client permission issues)\n",
    "check_jar_cmd = f\"gsutil -q stat gs://{BUCKET_NAME}/jars/spark-solr-4.0.0-shaded.jar\"\n",
    "jar_exists = subprocess.run(check_jar_cmd, shell=True).returncode == 0\n",
    "\n",
    "if not jar_exists:\n",
    "    print(\"Downloading spark-solr JAR (one-time setup)...\")\n",
    "    \n",
    "    # Create local jars directory\n",
    "    os.makedirs(\"jars\", exist_ok=True)\n",
    "    local_jar_path = \"jars/spark-solr-4.0.0-shaded.jar\"\n",
    "    \n",
    "    # Download JAR if not already local\n",
    "    if not os.path.exists(local_jar_path):\n",
    "        jar_url = \"https://repo1.maven.org/maven2/com/lucidworks/spark/spark-solr/4.0.0/spark-solr-4.0.0-shaded.jar\"\n",
    "        print(f\"  Downloading from Maven Central...\")\n",
    "        urllib.request.urlretrieve(jar_url, local_jar_path)\n",
    "        print(f\"  ✓ Downloaded to {local_jar_path}\")\n",
    "    \n",
    "    # Upload to GCS using gsutil\n",
    "    upload_cmd = f\"gsutil cp {local_jar_path} gs://{BUCKET_NAME}/jars/\"\n",
    "    result = subprocess.run(upload_cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ Uploaded JAR to gs://{BUCKET_NAME}/jars/\")\n",
    "    else:\n",
    "        print(f\"✗ Upload failed: {result.stderr}\")\n",
    "else:\n",
    "    print(f\"⏭️  JAR already exists at gs://{BUCKET_NAME}/jars/spark-solr-4.0.0-shaded.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbecf5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Solr Internal IP: 10.128.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://spark_job/index_to_solr_gcp.py [Content-Type=text/x-python]...\n",
      "/ [1 files][   1000 B/   1000 B]                                                \n",
      "Operation completed over 1 objects/1000.0 B.                                     \n",
      "/ [1 files][   1000 B/   1000 B]                                                \n",
      "Operation completed over 1 objects/1000.0 B.                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Uploaded job to gs://family-tree-469815-spark-solr-data/jobs/index_to_solr_gcp.py\n",
      "Submitting Spark job to Dataproc...\n",
      "Job [9e330051be42431ba95b7060b09f71c8] submitted.\n",
      "Waiting for job output...\n",
      "Job [9e330051be42431ba95b7060b09f71c8] submitted.\n",
      "Waiting for job output...\n",
      "25/11/25 02:40:07 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "25/11/25 02:40:07 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "25/11/25 02:40:08 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/11/25 02:40:08 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "25/11/25 02:40:08 INFO org.sparkproject.jetty.util.log: Logging initialized @4271ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "25/11/25 02:40:08 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_472-b08\n",
      "25/11/25 02:40:08 INFO org.sparkproject.jetty.server.Server: Started @4388ms\n",
      "25/11/25 02:40:08 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@1ef4a040{HTTP/1.1, (http/1.1)}{0.0.0.0:42337}\n",
      "25/11/25 02:40:07 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "25/11/25 02:40:07 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "25/11/25 02:40:08 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/11/25 02:40:08 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "25/11/25 02:40:08 INFO org.sparkproject.jetty.util.log: Logging initialized @4271ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "25/11/25 02:40:08 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_472-b08\n",
      "25/11/25 02:40:08 INFO org.sparkproject.jetty.server.Server: Started @4388ms\n",
      "25/11/25 02:40:08 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@1ef4a040{HTTP/1.1, (http/1.1)}{0.0.0.0:42337}\n",
      "25/11/25 02:40:09 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics\n",
      "25/11/25 02:40:09 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-solr-cluster-m/10.128.0.4:8032\n",
      "25/11/25 02:40:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at spark-solr-cluster-m/10.128.0.4:10200\n",
      "25/11/25 02:40:09 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics\n",
      "25/11/25 02:40:09 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-solr-cluster-m/10.128.0.4:8032\n",
      "25/11/25 02:40:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at spark-solr-cluster-m/10.128.0.4:10200\n",
      "25/11/25 02:40:10 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "25/11/25 02:40:10 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/11/25 02:40:10 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "25/11/25 02:40:10 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/11/25 02:40:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1764033530397_0006\n",
      "25/11/25 02:40:13 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-solr-cluster-m/10.128.0.4:8030\n",
      "25/11/25 02:40:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1764033530397_0006\n",
      "25/11/25 02:40:13 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-solr-cluster-m/10.128.0.4:8030\n",
      "25/11/25 02:40:15 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "25/11/25 02:40:15 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "Reading data from gs://family-tree-469815-spark-solr-data/data/dummy_data.json\n",
      "Reading data from gs://family-tree-469815-spark-solr-data/data/dummy_data.json\n",
      "Schema:\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- in_stock: boolean (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "Indexing to Solr collection 'dummy_data' via ZK '10.128.0.2:9983'...\n",
      "25/11/25 02:40:24 INFO com.lucidworks.spark.util.SolrSupport$: Creating a new SolrCloudClient for zkhost 10.128.0.2:9983\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.14-d25f337f749cadaac3b022158b9b6f3f2e919c8d, built on 01/10/1970 15:45 GMT\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:host.name=spark-solr-cluster-m.us-central1-a.c.family-tree-469815.internal\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.version=1.8.0_472\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.vendor=Temurin\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/temurin-8-jdk-amd64/jre\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.class.path=/usr/lib/spark/conf/:/usr/lib/spark/jars/commons-compiler-3.0.16.jar:/usr/lib/spark/jars/jetty-util-9.4.40.v20210413.jar:/usr/lib/spark/jars/accessors-smart-2.4.9.jar:/usr/lib/spark/jars/javax.inject-1.jar:/usr/lib/spark/jars/spire_2.12-0.17.0-M1.jar:/usr/lib/spark/jars/spark-unsafe_2.12-3.1.3.jar:/usr/lib/spark/jars/threeten-extra-1.5.0.jar:/usr/lib/spark/jars/scala-library-2.12.14.jar:/usr/lib/spark/jars/datanucleus-rdbms-4.1.19.jar:/usr/lib/spark/jars/gson-2.9.0.jar:/usr/lib/spark/jars/jersey-container-servlet-core-2.30.jar:/usr/lib/spark/jars/RoaringBitmap-0.9.0.jar:/usr/lib/spark/jars/jetty-util-ajax-9.4.40.v20210413.jar:/usr/lib/spark/jars/minlog-1.3.0.jar:/usr/lib/spark/jars/arrow-memory-netty-2.0.0.jar:/usr/lib/spark/jars/parquet-hadoop-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/spark-launcher_2.12-3.1.3.jar:/usr/lib/spark/jars/chill-java-0.9.5.jar:/usr/lib/spark/jars/spark-hadoop-cloud_2.12-3.1.3.jar:/usr/lib/spark/jars/velocity-1.5.jar:/usr/lib/spark/jars/xbean-asm7-shaded-4.15.jar:/usr/lib/spark/jars/slf4j-api-1.7.36.jar:/usr/lib/spark/jars/commons-daemon-1.0.13.jar:/usr/lib/spark/jars/jsp-api-2.1.jar:/usr/lib/spark/jars/datanucleus-core-4.1.17.jar:/usr/lib/spark/jars/kubernetes-model-certificates-4.12.0.jar:/usr/lib/spark/jars/bcprov-jdk15on-1.60.jar:/usr/lib/spark/jars/datanucleus-api-jdo-4.2.4.jar:/usr/lib/spark/jars/kubernetes-model-coordination-4.12.0.jar:/usr/lib/spark/jars/algebra_2.12-2.0.0-M2.jar:/usr/lib/spark/jars/curator-recipes-2.13.0.jar:/usr/lib/spark/jars/kubernetes-model-extensions-4.12.0.jar:/usr/lib/spark/jars/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/lib/spark/jars/spark-sketch_2.12-3.1.3.jar:/usr/lib/spark/jars/janino-3.0.16.jar:/usr/lib/spark/jars/commons-codec-1.10.jar:/usr/lib/spark/jars/kerb-crypto-1.0.1.jar:/usr/lib/spark/jars/hadoop-azure-datalake-3.2.4.jar:/usr/lib/spark/jars/shuffle-endpoints-1.1.0.jar:/usr/lib/spark/jars/hadoop-auth-3.2.4.jar:/usr/lib/spark/jars/spark-mllib-local_2.12-3.1.3.jar:/usr/lib/spark/jars/avro-mapred-1.9.2.jar:/usr/lib/spark/jars/jul-to-slf4j-1.7.36.jar:/usr/lib/spark/jars/jodd-core-3.5.2.jar:/usr/lib/spark/jars/hive-shims-2.3.8.jar:/usr/lib/spark/jars/kubernetes-client-4.12.0.jar:/usr/lib/spark/jars/breeze-macros_2.12-1.0.jar:/usr/lib/spark/jars/kubernetes-model-apps-4.12.0.jar:/usr/lib/spark/jars/ini4j-0.5.4.jar:/usr/lib/spark/jars/bonecp-0.8.0.RELEASE.jar:/usr/lib/spark/jars/hk2-locator-2.6.1.jar:/usr/lib/spark/jars/azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/spark/jars/kerby-asn1-1.0.1.jar:/usr/lib/spark/jars/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/spark/jars/netlib-native_system-linux-x86_64-1.1-natives.jar:/usr/lib/spark/jars/hive-storage-api-2.7.2.jar:/usr/lib/spark/jars/opencsv-2.3.jar:/usr/lib/spark/jars/hadoop-lzo-0.4.20.jar:/usr/lib/spark/jars/antlr-runtime-3.5.2.jar:/usr/lib/spark/jars/hadoop-yarn-common-3.2.4.jar:/usr/lib/spark/jars/flatbuffers-java-1.9.0.jar:/usr/lib/spark/jars/hive-llap-common-2.3.8.jar:/usr/lib/spark/jars/parquet-encoding-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/shims-0.9.0.jar:/usr/lib/spark/jars/avro-1.9.2.jar:/usr/lib/spark/jars/jettison-1.1.jar:/usr/lib/spark/jars/netlib-native_system-linux-armhf-1.1-natives.jar:/usr/lib/spark/jars/istack-commons-runtime-3.0.8.jar:/usr/lib/spark/jars/hive-beeline-2.3.8.jar:/usr/lib/spark/jars/orc-mapreduce-1.5.13.jar:/usr/lib/spark/jars/stax2-api-4.2.1.jar:/usr/lib/spark/jars/token-provider-1.0.1.jar:/usr/lib/spark/jars/compress-lzf-1.0.3.jar:/usr/lib/spark/jars/aopalliance-1.0.jar:/usr/lib/spark/jars/guava-28.2-jre.jar:/usr/lib/spark/jars/jackson-annotations-2.10.5.jar:/usr/lib/spark/jars/jackson-dataformat-yaml-2.10.5.jar:/usr/lib/spark/jars/okhttp-3.12.12.jar:/usr/lib/spark/jars/avro-ipc-jetty-1.9.2.jar:/usr/lib/spark/jars/hadoop-cloud-storage-3.2.4.jar:/usr/lib/spark/jars/JTransforms-3.1.jar:/usr/lib/spark/jars/kubernetes-model-autoscaling-4.12.0.jar:/usr/lib/spark/jars/javax.activation-api-1.2.0.jar:/usr/lib/spark/jars/hadoop-google-secret-manager-credential-provider-3.2.4.jar:/usr/lib/spark/jars/hadoop-yarn-client-3.2.4.jar:/usr/lib/spark/jars/aliyun-sdk-oss-3.13.0.jar:/usr/lib/spark/jars/spire-macros_2.12-0.17.0-M1.jar:/usr/lib/spark/jars/hive-serde-2.3.8.jar:/usr/lib/spark/jars/hive-shims-0.23-2.3.8.jar:/usr/lib/spark/jars/spark-events-1.0.0.jar:/usr/lib/spark/jars/netlib-native_system-win-x86_64-1.1-natives.jar:/usr/lib/spark/jars/httpclient-4.5.6.jar:/usr/lib/spark/jars/commons-configuration2-2.1.1.jar:/usr/lib/spark/jars/jaxb-runtime-2.3.2.jar:/usr/lib/spark/jars/azure-storage-7.0.0.jar:/usr/lib/spark/jars/ivy-2.5.1.jar:/usr/lib/spark/jars/hadoop-yarn-server-common-3.2.4.jar:/usr/lib/spark/jars/j2objc-annotations-1.3.jar:/usr/lib/spark/jars/aliyun-java-sdk-kms-2.11.0.jar:/usr/lib/spark/jars/spark-yarn_2.12-3.1.3.jar:/usr/lib/spark/jars/hive-service-rpc-3.1.2.jar:/usr/lib/spark/jars/jniloader-1.1.jar:/usr/lib/spark/jars/arrow-format-2.0.0.jar:/usr/lib/spark/jars/spark-network-shuffle_2.12-3.1.3.jar:/usr/lib/spark/jars/jta-1.1.jar:/usr/lib/spark/jars/kubernetes-model-events-4.12.0.jar:/usr/lib/spark/jars/commons-cli-1.2.jar:/usr/lib/spark/jars/scala-reflect-2.12.14.jar:/usr/lib/spark/jars/JLargeArrays-1.5.jar:/usr/lib/spark/jars/parquet-jackson-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/netlib-native_ref-win-x86_64-1.1-natives.jar:/usr/lib/spark/jars/scala-parser-combinators_2.12-1.1.2.jar:/usr/lib/spark/jars/jackson-jaxrs-base-2.10.5.jar:/usr/lib/spark/jars/guice-4.0.jar:/usr/lib/spark/jars/commons-collections-3.2.2.jar:/usr/lib/spark/jars/hadoop-yarn-registry-3.2.4.jar:/usr/lib/spark/jars/aws-java-sdk-bundle-1.11.901.jar:/usr/lib/spark/jars/zjsonpatch-0.3.0.jar:/usr/lib/spark/jars/orc-core-1.5.13.jar:/usr/lib/spark/jars/kubernetes-model-apiextensions-4.12.0.jar:/usr/lib/spark/jars/aopalliance-repackaged-2.6.1.jar:/usr/lib/spark/jars/hadoop-mapreduce-client-jobclient-3.2.4.jar:/usr/lib/spark/jars/scala-compiler-2.12.14.jar:/usr/lib/spark/jars/hive-cli-2.3.8.jar:/usr/lib/spark/jars/kerb-simplekdc-1.0.1.jar:/usr/lib/spark/jars/libfb303-0.9.3.jar:/usr/lib/spark/jars/ST4-4.0.4.jar:/usr/lib/spark/jars/audience-annotations-0.5.0.jar:/usr/lib/spark/jars/aircompressor-0.10.jar:/usr/lib/spark/jars/kerb-identity-1.0.1.jar:/usr/lib/spark/jars/jersey-client-2.30.jar:/usr/lib/spark/jars/kubernetes-model-core-4.12.0.jar:/usr/lib/spark/jars/scala-xml_2.12-1.2.0.jar:/usr/lib/spark/jars/org.jacoco.agent-0.8.5-runtime.jar:/usr/lib/spark/jars/transaction-api-1.1.jar:/usr/lib/spark/jars/kubernetes-model-storageclass-4.12.0.jar:/usr/lib/spark/jars/netty-all-4.1.51.Final.jar:/usr/lib/spark/jars/py4j-0.10.9.jar:/usr/lib/spark/jars/hk2-utils-2.6.1.jar:/usr/lib/spark/jars/commons-beanutils-1.9.4.jar:/usr/lib/spark/jars/spark-tags_2.12-3.1.3-tests.jar:/usr/lib/spark/jars/chill_2.12-0.9.5.jar:/usr/lib/spark/jars/netlib-native_ref-linux-i686-1.1-natives.jar:/usr/lib/spark/jars/httpcore-4.4.12.jar:/usr/lib/spark/jars/aliyun-java-sdk-ram-3.1.0.jar:/usr/lib/spark/jars/hadoop-yarn-api-3.2.4.jar:/usr/lib/spark/jars/hk2-api-2.6.1.jar:/usr/lib/spark/jars/jakarta.servlet-api-4.0.3.jar:/usr/lib/spark/jars/derby-10.12.1.1.jar:/usr/lib/spark/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/spark/jars/curator-client-2.13.0.jar:/usr/lib/spark/jars/nimbus-jose-jwt-9.8.1.jar:/usr/lib/spark/jars/osgi-resource-locator-1.0.3.jar:/usr/lib/spark/jars/snappy-java-1.1.10.4.jar:/usr/lib/spark/jars/orc-shims-1.5.13.jar:/usr/lib/spark/jars/breeze_2.12-1.0.jar:/usr/lib/spark/jars/spark-graphx_2.12-3.1.3.jar:/usr/lib/spark/jars/kubernetes-model-scheduling-4.12.0.jar:/usr/lib/spark/jars/native_ref-java-1.1.jar:/usr/lib/spark/jars/activation-1.1.1.jar:/usr/lib/spark/jars/spark-hive_2.12-3.1.3.jar:/usr/lib/spark/jars/generex-1.0.2.jar:/usr/lib/spark/jars/json4s-core_2.12-3.7.0-M5.jar:/usr/lib/spark/jars/antlr4-runtime-4.8-1.jar:/usr/lib/spark/jars/kerb-util-1.0.1.jar:/usr/lib/spark/jars/objenesis-2.6.jar:/usr/lib/spark/jars/metrics-jvm-4.1.1.jar:/usr/lib/spark/jars/opentracing-api-0.33.0.jar:/usr/lib/spark/jars/checker-qual-2.10.0.jar:/usr/lib/spark/jars/jdom2-2.0.6.jar:/usr/lib/spark/jars/hadoop-hdfs-client-3.2.4.jar:/usr/lib/spark/jars/netlib-native_ref-linux-x86_64-1.1-natives.jar:/usr/lib/spark/jars/jackson-core-asl-1.9.13.jar:/usr/lib/spark/jars/jersey-common-2.30.jar:/usr/lib/spark/jars/hadoop-mapreduce-client-core-3.2.4.jar:/usr/lib/spark/jars/azure-keyvault-core-1.0.0.jar:/usr/lib/spark/jars/javolution-5.5.1.jar:/usr/lib/spark/jars/json4s-ast_2.12-3.7.0-M5.jar:/usr/lib/spark/jars/hadoop-mapreduce-client-common-3.2.4.jar:/usr/lib/spark/jars/commons-dbcp-1.4.jar:/usr/lib/spark/jars/bcpkix-jdk15on-1.60.jar:/usr/lib/spark/jars/jcip-annotations-1.0-1.jar:/usr/lib/spark/jars/hive-exec-2.3.8-core.jar:/usr/lib/spark/jars/woodstox-core-5.4.0.jar:/usr/lib/spark/jars/kerb-admin-1.0.1.jar:/usr/lib/spark/jars/zstd-jni-1.4.8-1.jar:/usr/lib/spark/jars/HikariCP-2.5.1.jar:/usr/lib/spark/jars/hbase-spark-1.0.1-SNAPSHOT.jar:/usr/lib/spark/jars/spark-sql_2.12-3.1.3.jar:/usr/lib/spark/jars/dnsjava-2.1.7.jar:/usr/lib/spark/jars/okio-1.14.0.jar:/usr/lib/spark/jars/cats-kernel_2.12-2.0.0-M4.jar:/usr/lib/spark/jars/spark-mllib_2.12-3.1.3.jar:/usr/lib/spark/jars/jdo-api-3.0.1.jar:/usr/lib/spark/jars/jakarta.inject-2.6.1.jar:/usr/lib/spark/jars/metrics-core-4.1.1.jar:/usr/lib/spark/jars/hadoop-openstack-3.2.4.jar:/usr/lib/spark/jars/jackson-module-paranamer-2.10.5.jar:/usr/lib/spark/jars/dataproc-spark-plugins-1.0.0.jar:/usr/lib/spark/jars/commons-text-1.10.0.jar:/usr/lib/spark/jars/hive-metastore-2.3.8.jar:/usr/lib/spark/jars/jakarta.annotation-api-1.3.5.jar:/usr/lib/spark/jars/kubernetes-model-policy-4.12.0.jar:/usr/lib/spark/jars/jackson-databind-2.10.5.jar:/usr/lib/spark/jars/re2j-1.1.jar:/usr/lib/spark/jars/commons-lang3-3.10.jar:/usr/lib/spark/jars/spire-util_2.12-0.17.0-M1.jar:/usr/lib/spark/jars/commons-compress-1.21.jar:/usr/lib/spark/jars/jakarta.activation-api-1.2.1.jar:/usr/lib/spark/jars/arrow-memory-core-2.0.0.jar:/usr/lib/spark/jars/spark-repl_2.12-3.1.3.jar:/usr/lib/spark/jars/protobuf-java-2.5.0.jar:/usr/lib/spark/jars/commons-logging-1.1.3.jar:/usr/lib/spark/jars/netlib-native_ref-linux-armhf-1.1-natives.jar:/usr/lib/spark/jars/jackson-dataformat-cbor-2.10.5.jar:/usr/lib/spark/jars/macro-compat_2.12-1.1.1.jar:/usr/lib/spark/jars/hbase-spark-protocol-shaded-1.0.1-SNAPSHOT.jar:/usr/lib/spark/jars/kryo-shaded-4.0.2.jar:/usr/lib/spark/jars/kubernetes-model-common-4.12.0.jar:/usr/lib/spark/jars/arpack_combined_all-0.1.jar:/usr/lib/spark/jars/commons-net-3.1.jar:/usr/lib/spark/jars/hadoop-aws-3.2.4.jar:/usr/lib/spark/jars/commons-lang-2.6.jar:/usr/lib/spark/jars/spark-streaming_2.12-3.1.3.jar:/usr/lib/spark/jars/jackson-module-scala_2.12-2.10.5.jar:/usr/lib/spark/jars/super-csv-2.2.0.jar:/usr/lib/spark/jars/kerb-common-1.0.1.jar:/usr/lib/spark/jars/netlib-native_system-linux-i686-1.1-natives.jar:/usr/lib/spark/jars/kubernetes-model-discovery-4.12.0.jar:/usr/lib/spark/jars/netlib-native_ref-osx-x86_64-1.1-natives.jar:/usr/lib/spark/jars/machinist_2.12-0.6.8.jar:/usr/lib/spark/jars/zookeeper-3.4.14.jar:/usr/lib/spark/jars/univocity-parsers-2.9.1.jar:/usr/lib/spark/jars/kerby-pkix-1.0.1.jar:/usr/lib/spark/jars/jakarta.validation-api-2.0.2.jar:/usr/lib/spark/jars/jersey-media-jaxb-2.30.jar:/usr/lib/spark/jars/core-1.1.2.jar:/usr/lib/spark/jars/hive-shims-scheduler-2.3.8.jar:/usr/lib/spark/jars/jsr305-3.0.0.jar:/usr/lib/spark/jars/kubernetes-model-rbac-4.12.0.jar:/usr/lib/spark/jars/json4s-scalap_2.12-3.7.0-M5.jar:/usr/lib/spark/jars/oro-2.0.8.jar:/usr/lib/spark/jars/snakeyaml-2.0.jar:/usr/lib/spark/jars/logging-interceptor-3.12.12.jar:/usr/lib/spark/jars/hadoop-yarn-server-web-proxy-3.2.4.jar:/usr/lib/spark/jars/hadoop-client-3.2.4.jar:/usr/lib/spark/jars/pyrolite-4.30.jar:/usr/lib/spark/jars/jpam-1.1.jar:/usr/lib/spark/jars/netlib-native_system-win-i686-1.1-natives.jar:/usr/lib/spark/jars/kubernetes-model-settings-4.12.0.jar:/usr/lib/spark/jars/slf4j-reload4j-1.7.36.jar:/usr/lib/spark/jars/kubernetes-model-admissionregistration-4.12.0.jar:/usr/lib/spark/jars/spark-kvstore_2.12-3.1.3.jar:/usr/lib/spark/jars/kerby-xdr-1.0.1.jar:/usr/lib/spark/jars/automaton-1.11-8.jar:/usr/lib/spark/jars/guice-servlet-4.0.jar:/usr/lib/spark/jars/kerb-server-1.0.1.jar:/usr/lib/spark/jars/kubernetes-model-batch-4.12.0.jar:/usr/lib/spark/jars/commons-pool-1.5.4.jar:/usr/lib/spark/jars/jersey-server-2.30.jar:/usr/lib/spark/jars/libthrift-0.12.0.jar:/usr/lib/spark/jars/failureaccess-1.0.1.jar:/usr/lib/spark/jars/tink-1.6.0.jar:/usr/lib/spark/jars/netlib-native_ref-win-i686-1.1-natives.jar:/usr/lib/spark/jars/jline-2.14.6.jar:/usr/lib/spark/jars/spark-tags_2.12-3.1.3.jar:/usr/lib/spark/jars/metrics-graphite-4.1.1.jar:/usr/lib/spark/jars/reload4j-1.2.22.jar:/usr/lib/spark/jars/paranamer-2.8.jar:/usr/lib/spark/jars/parquet-common-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/kerby-util-1.0.1.jar:/usr/lib/spark/jars/htrace-core4-4.1.0-incubating.jar:/usr/lib/spark/jars/javax.annotation-api-1.3.2.jar:/usr/lib/spark/jars/hadoop-common-3.2.4.jar:/usr/lib/spark/jars/native_system-java-1.1.jar:/usr/lib/spark/jars/okhttp-2.7.5.jar:/usr/lib/spark/jars/scala-collection-compat_2.12-2.1.1.jar:/usr/lib/spark/jars/netlib-native_system-osx-x86_64-1.1-natives.jar:/usr/lib/spark/jars/stream-2.9.6.jar:/usr/lib/spark/jars/kerb-core-1.0.1.jar:/usr/lib/spark/jars/spire-platform_2.12-0.17.0-M1.jar:/usr/lib/spark/jars/metrics-jmx-4.1.1.jar:/usr/lib/spark/jars/json-smart-2.4.9.jar:/usr/lib/spark/jars/wildfly-openssl-1.0.7.Final.jar:/usr/lib/spark/jars/hadoop-azure-3.2.4.jar:/usr/lib/spark/jars/commons-httpclient-3.1.jar:/usr/lib/spark/jars/spark-core_2.12-3.1.3.jar:/usr/lib/spark/jars/hive-shims-common-2.3.8.jar:/usr/lib/spark/jars/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/spark/jars/commons-io-2.5.jar:/usr/lib/spark/jars/commons-crypto-1.1.0.jar:/usr/lib/spark/jars/curator-framework-2.13.0.jar:/usr/lib/spark/jars/json4s-jackson_2.12-3.7.0-M5.jar:/usr/lib/spark/jars/javassist-3.25.0-GA.jar:/usr/lib/spark/jars/javax.jdo-3.2.0-m3.jar:/usr/lib/spark/jars/hive-vector-code-gen-2.3.8.jar:/usr/lib/spark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/usr/lib/spark/jars/jaxb-api-2.2.11.jar:/usr/lib/spark/jars/shapeless_2.12-2.3.3.jar:/usr/lib/spark/jars/kerby-config-1.0.1.jar:/usr/lib/spark/jars/aliyun-java-sdk-core-4.5.10.jar:/usr/lib/spark/jars/kubernetes-model-networking-4.12.0.jar:/usr/lib/spark/jars/hbase-spark-protocol-1.0.1-SNAPSHOT.jar:/usr/lib/spark/jars/jackson-mapper-asl-1.9.13.jar:/usr/lib/spark/jars/jersey-container-servlet-2.30.jar:/usr/lib/spark/jars/metrics-json-4.1.1.jar:/usr/lib/spark/jars/error_prone_annotations-2.3.4.jar:/usr/lib/spark/jars/spark-hive-thriftserver_2.12-3.1.3.jar:/usr/lib/spark/jars/hive-jdbc-2.3.8.jar:/usr/lib/spark/jars/jakarta.ws.rs-api-2.1.6.jar:/usr/lib/spark/jars/parquet-format-structures-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/jackson-datatype-jsr310-2.11.2.jar:/usr/lib/spark/jars/spark-catalyst_2.12-3.1.3.jar:/usr/lib/spark/jars/jersey-hk2-2.30.jar:/usr/lib/spark/jars/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/spark/jars/kerb-client-1.0.1.jar:/usr/lib/spark/jars/json-1.8.jar:/usr/lib/spark/jars/velocity-engine-core-2.2.jar:/usr/lib/spark/jars/stax-api-1.0.1.jar:/usr/lib/spark/jars/spark-network-common_2.12-3.1.3.jar:/usr/lib/spark/jars/joda-time-2.10.5.jar:/usr/lib/spark/jars/arrow-vector-2.0.0.jar:/usr/lib/spark/jars/lz4-java-1.7.1.jar:/usr/lib/spark/jars/hadoop-aliyun-3.2.4.jar:/usr/lib/spark/jars/commons-math3-3.4.1.jar:/usr/lib/spark/jars/hive-common-2.3.8.jar:/usr/lib/spark/jars/parquet-column-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/hadoop-annotations-3.2.4.jar:/usr/lib/spark/jars/jcl-over-slf4j-1.7.36.jar:/usr/lib/spark/jars/leveldbjni-all-1.8.jar:/usr/lib/spark/jars/kubernetes-model-metrics-4.12.0.jar:/usr/lib/spark/jars/ehcache-3.3.1.jar:/usr/lib/spark/jars/avro-ipc-1.9.2.jar:/usr/lib/spark/jars/spark-kubernetes_2.12-3.1.3.jar:/usr/lib/spark/jars/jackson-core-2.10.5.jar:/usr/lib/spark/jars/opentracing-noop-0.33.0.jar:/usr/lib/spark/jars/opentracing-util-0.33.0.jar:/etc/hadoop/conf/:/etc/hive/conf/:/usr/local/share/google/dataproc/lib/spark-metrics-listener.jar:/usr/local/share/google/dataproc/lib/ranger_gcs_plugin_client.jar:/usr/local/share/google/dataproc/lib/bigquery-metastore-client-0.0.20251014-jar-with-dependencies.jar:/usr/local/share/google/dataproc/lib/spark-metrics-listener-dataproc-2.0-1.15.0.jar:/usr/local/share/google/dataproc/lib/gcs-connector-hadoop3-2.2.29.jar:/usr/local/share/google/dataproc/lib/gcs-connector.jar:/usr/share/java/mysql.jar\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.library.path=:/usr/lib/hadoop/lib/native:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.compiler=<NA>\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.name=Linux\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.arch=amd64\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.version=5.10.0-0.deb10.16-cloud-amd64\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.name=root\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.home=/root\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.dir=/tmp/9e330051be42431ba95b7060b09f71c8\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=10.128.0.2:9983 sessionTimeout=30000 watcher=org.apache.solr.common.cloud.SolrZkClient$ProcessWatchWithExecutor@13ab7bb\n",
      "25/11/25 02:40:24 INFO org.apache.solr.common.cloud.ConnectionManager: Waiting for client to connect to ZooKeeper\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server solr-instance.us-central1-a.c.family-tree-469815.internal/10.128.0.2:9983. Will not attempt to authenticate using SASL (unknown error)\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to solr-instance.us-central1-a.c.family-tree-469815.internal/10.128.0.2:9983, initiating session\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server solr-instance.us-central1-a.c.family-tree-469815.internal/10.128.0.2:9983, sessionid = 0x10000064e510002, negotiated timeout = 30000\n",
      "25/11/25 02:40:24 INFO org.apache.solr.common.cloud.ConnectionManager: zkClient has connected\n",
      "25/11/25 02:40:24 INFO org.apache.solr.common.cloud.ConnectionManager: Client is connected to ZooKeeper\n",
      "25/11/25 02:40:24 INFO org.apache.solr.common.cloud.ZkStateReader: Updated live nodes from ZooKeeper... (0) -> (1)\n",
      "25/11/25 02:40:24 INFO org.apache.solr.client.solrj.impl.ZkClientClusterStateProvider: Cluster at 10.128.0.2:9983 ready\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> author, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> string\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> category, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> string\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> created_at, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> string\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> description, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> string\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> in_stock, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> boolean\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> price, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> pdouble\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> title, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> string\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: Sending request to Solr schema API to add 7 fields.\n",
      "Schema:\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- in_stock: boolean (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "Indexing to Solr collection 'dummy_data' via ZK '10.128.0.2:9983'...\n",
      "25/11/25 02:40:24 INFO com.lucidworks.spark.util.SolrSupport$: Creating a new SolrCloudClient for zkhost 10.128.0.2:9983\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.14-d25f337f749cadaac3b022158b9b6f3f2e919c8d, built on 01/10/1970 15:45 GMT\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:host.name=spark-solr-cluster-m.us-central1-a.c.family-tree-469815.internal\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.version=1.8.0_472\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.vendor=Temurin\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/temurin-8-jdk-amd64/jre\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.class.path=/usr/lib/spark/conf/:/usr/lib/spark/jars/commons-compiler-3.0.16.jar:/usr/lib/spark/jars/jetty-util-9.4.40.v20210413.jar:/usr/lib/spark/jars/accessors-smart-2.4.9.jar:/usr/lib/spark/jars/javax.inject-1.jar:/usr/lib/spark/jars/spire_2.12-0.17.0-M1.jar:/usr/lib/spark/jars/spark-unsafe_2.12-3.1.3.jar:/usr/lib/spark/jars/threeten-extra-1.5.0.jar:/usr/lib/spark/jars/scala-library-2.12.14.jar:/usr/lib/spark/jars/datanucleus-rdbms-4.1.19.jar:/usr/lib/spark/jars/gson-2.9.0.jar:/usr/lib/spark/jars/jersey-container-servlet-core-2.30.jar:/usr/lib/spark/jars/RoaringBitmap-0.9.0.jar:/usr/lib/spark/jars/jetty-util-ajax-9.4.40.v20210413.jar:/usr/lib/spark/jars/minlog-1.3.0.jar:/usr/lib/spark/jars/arrow-memory-netty-2.0.0.jar:/usr/lib/spark/jars/parquet-hadoop-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/spark-launcher_2.12-3.1.3.jar:/usr/lib/spark/jars/chill-java-0.9.5.jar:/usr/lib/spark/jars/spark-hadoop-cloud_2.12-3.1.3.jar:/usr/lib/spark/jars/velocity-1.5.jar:/usr/lib/spark/jars/xbean-asm7-shaded-4.15.jar:/usr/lib/spark/jars/slf4j-api-1.7.36.jar:/usr/lib/spark/jars/commons-daemon-1.0.13.jar:/usr/lib/spark/jars/jsp-api-2.1.jar:/usr/lib/spark/jars/datanucleus-core-4.1.17.jar:/usr/lib/spark/jars/kubernetes-model-certificates-4.12.0.jar:/usr/lib/spark/jars/bcprov-jdk15on-1.60.jar:/usr/lib/spark/jars/datanucleus-api-jdo-4.2.4.jar:/usr/lib/spark/jars/kubernetes-model-coordination-4.12.0.jar:/usr/lib/spark/jars/algebra_2.12-2.0.0-M2.jar:/usr/lib/spark/jars/curator-recipes-2.13.0.jar:/usr/lib/spark/jars/kubernetes-model-extensions-4.12.0.jar:/usr/lib/spark/jars/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/lib/spark/jars/spark-sketch_2.12-3.1.3.jar:/usr/lib/spark/jars/janino-3.0.16.jar:/usr/lib/spark/jars/commons-codec-1.10.jar:/usr/lib/spark/jars/kerb-crypto-1.0.1.jar:/usr/lib/spark/jars/hadoop-azure-datalake-3.2.4.jar:/usr/lib/spark/jars/shuffle-endpoints-1.1.0.jar:/usr/lib/spark/jars/hadoop-auth-3.2.4.jar:/usr/lib/spark/jars/spark-mllib-local_2.12-3.1.3.jar:/usr/lib/spark/jars/avro-mapred-1.9.2.jar:/usr/lib/spark/jars/jul-to-slf4j-1.7.36.jar:/usr/lib/spark/jars/jodd-core-3.5.2.jar:/usr/lib/spark/jars/hive-shims-2.3.8.jar:/usr/lib/spark/jars/kubernetes-client-4.12.0.jar:/usr/lib/spark/jars/breeze-macros_2.12-1.0.jar:/usr/lib/spark/jars/kubernetes-model-apps-4.12.0.jar:/usr/lib/spark/jars/ini4j-0.5.4.jar:/usr/lib/spark/jars/bonecp-0.8.0.RELEASE.jar:/usr/lib/spark/jars/hk2-locator-2.6.1.jar:/usr/lib/spark/jars/azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/spark/jars/kerby-asn1-1.0.1.jar:/usr/lib/spark/jars/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/spark/jars/netlib-native_system-linux-x86_64-1.1-natives.jar:/usr/lib/spark/jars/hive-storage-api-2.7.2.jar:/usr/lib/spark/jars/opencsv-2.3.jar:/usr/lib/spark/jars/hadoop-lzo-0.4.20.jar:/usr/lib/spark/jars/antlr-runtime-3.5.2.jar:/usr/lib/spark/jars/hadoop-yarn-common-3.2.4.jar:/usr/lib/spark/jars/flatbuffers-java-1.9.0.jar:/usr/lib/spark/jars/hive-llap-common-2.3.8.jar:/usr/lib/spark/jars/parquet-encoding-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/shims-0.9.0.jar:/usr/lib/spark/jars/avro-1.9.2.jar:/usr/lib/spark/jars/jettison-1.1.jar:/usr/lib/spark/jars/netlib-native_system-linux-armhf-1.1-natives.jar:/usr/lib/spark/jars/istack-commons-runtime-3.0.8.jar:/usr/lib/spark/jars/hive-beeline-2.3.8.jar:/usr/lib/spark/jars/orc-mapreduce-1.5.13.jar:/usr/lib/spark/jars/stax2-api-4.2.1.jar:/usr/lib/spark/jars/token-provider-1.0.1.jar:/usr/lib/spark/jars/compress-lzf-1.0.3.jar:/usr/lib/spark/jars/aopalliance-1.0.jar:/usr/lib/spark/jars/guava-28.2-jre.jar:/usr/lib/spark/jars/jackson-annotations-2.10.5.jar:/usr/lib/spark/jars/jackson-dataformat-yaml-2.10.5.jar:/usr/lib/spark/jars/okhttp-3.12.12.jar:/usr/lib/spark/jars/avro-ipc-jetty-1.9.2.jar:/usr/lib/spark/jars/hadoop-cloud-storage-3.2.4.jar:/usr/lib/spark/jars/JTransforms-3.1.jar:/usr/lib/spark/jars/kubernetes-model-autoscaling-4.12.0.jar:/usr/lib/spark/jars/javax.activation-api-1.2.0.jar:/usr/lib/spark/jars/hadoop-google-secret-manager-credential-provider-3.2.4.jar:/usr/lib/spark/jars/hadoop-yarn-client-3.2.4.jar:/usr/lib/spark/jars/aliyun-sdk-oss-3.13.0.jar:/usr/lib/spark/jars/spire-macros_2.12-0.17.0-M1.jar:/usr/lib/spark/jars/hive-serde-2.3.8.jar:/usr/lib/spark/jars/hive-shims-0.23-2.3.8.jar:/usr/lib/spark/jars/spark-events-1.0.0.jar:/usr/lib/spark/jars/netlib-native_system-win-x86_64-1.1-natives.jar:/usr/lib/spark/jars/httpclient-4.5.6.jar:/usr/lib/spark/jars/commons-configuration2-2.1.1.jar:/usr/lib/spark/jars/jaxb-runtime-2.3.2.jar:/usr/lib/spark/jars/azure-storage-7.0.0.jar:/usr/lib/spark/jars/ivy-2.5.1.jar:/usr/lib/spark/jars/hadoop-yarn-server-common-3.2.4.jar:/usr/lib/spark/jars/j2objc-annotations-1.3.jar:/usr/lib/spark/jars/aliyun-java-sdk-kms-2.11.0.jar:/usr/lib/spark/jars/spark-yarn_2.12-3.1.3.jar:/usr/lib/spark/jars/hive-service-rpc-3.1.2.jar:/usr/lib/spark/jars/jniloader-1.1.jar:/usr/lib/spark/jars/arrow-format-2.0.0.jar:/usr/lib/spark/jars/spark-network-shuffle_2.12-3.1.3.jar:/usr/lib/spark/jars/jta-1.1.jar:/usr/lib/spark/jars/kubernetes-model-events-4.12.0.jar:/usr/lib/spark/jars/commons-cli-1.2.jar:/usr/lib/spark/jars/scala-reflect-2.12.14.jar:/usr/lib/spark/jars/JLargeArrays-1.5.jar:/usr/lib/spark/jars/parquet-jackson-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/netlib-native_ref-win-x86_64-1.1-natives.jar:/usr/lib/spark/jars/scala-parser-combinators_2.12-1.1.2.jar:/usr/lib/spark/jars/jackson-jaxrs-base-2.10.5.jar:/usr/lib/spark/jars/guice-4.0.jar:/usr/lib/spark/jars/commons-collections-3.2.2.jar:/usr/lib/spark/jars/hadoop-yarn-registry-3.2.4.jar:/usr/lib/spark/jars/aws-java-sdk-bundle-1.11.901.jar:/usr/lib/spark/jars/zjsonpatch-0.3.0.jar:/usr/lib/spark/jars/orc-core-1.5.13.jar:/usr/lib/spark/jars/kubernetes-model-apiextensions-4.12.0.jar:/usr/lib/spark/jars/aopalliance-repackaged-2.6.1.jar:/usr/lib/spark/jars/hadoop-mapreduce-client-jobclient-3.2.4.jar:/usr/lib/spark/jars/scala-compiler-2.12.14.jar:/usr/lib/spark/jars/hive-cli-2.3.8.jar:/usr/lib/spark/jars/kerb-simplekdc-1.0.1.jar:/usr/lib/spark/jars/libfb303-0.9.3.jar:/usr/lib/spark/jars/ST4-4.0.4.jar:/usr/lib/spark/jars/audience-annotations-0.5.0.jar:/usr/lib/spark/jars/aircompressor-0.10.jar:/usr/lib/spark/jars/kerb-identity-1.0.1.jar:/usr/lib/spark/jars/jersey-client-2.30.jar:/usr/lib/spark/jars/kubernetes-model-core-4.12.0.jar:/usr/lib/spark/jars/scala-xml_2.12-1.2.0.jar:/usr/lib/spark/jars/org.jacoco.agent-0.8.5-runtime.jar:/usr/lib/spark/jars/transaction-api-1.1.jar:/usr/lib/spark/jars/kubernetes-model-storageclass-4.12.0.jar:/usr/lib/spark/jars/netty-all-4.1.51.Final.jar:/usr/lib/spark/jars/py4j-0.10.9.jar:/usr/lib/spark/jars/hk2-utils-2.6.1.jar:/usr/lib/spark/jars/commons-beanutils-1.9.4.jar:/usr/lib/spark/jars/spark-tags_2.12-3.1.3-tests.jar:/usr/lib/spark/jars/chill_2.12-0.9.5.jar:/usr/lib/spark/jars/netlib-native_ref-linux-i686-1.1-natives.jar:/usr/lib/spark/jars/httpcore-4.4.12.jar:/usr/lib/spark/jars/aliyun-java-sdk-ram-3.1.0.jar:/usr/lib/spark/jars/hadoop-yarn-api-3.2.4.jar:/usr/lib/spark/jars/hk2-api-2.6.1.jar:/usr/lib/spark/jars/jakarta.servlet-api-4.0.3.jar:/usr/lib/spark/jars/derby-10.12.1.1.jar:/usr/lib/spark/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/spark/jars/curator-client-2.13.0.jar:/usr/lib/spark/jars/nimbus-jose-jwt-9.8.1.jar:/usr/lib/spark/jars/osgi-resource-locator-1.0.3.jar:/usr/lib/spark/jars/snappy-java-1.1.10.4.jar:/usr/lib/spark/jars/orc-shims-1.5.13.jar:/usr/lib/spark/jars/breeze_2.12-1.0.jar:/usr/lib/spark/jars/spark-graphx_2.12-3.1.3.jar:/usr/lib/spark/jars/kubernetes-model-scheduling-4.12.0.jar:/usr/lib/spark/jars/native_ref-java-1.1.jar:/usr/lib/spark/jars/activation-1.1.1.jar:/usr/lib/spark/jars/spark-hive_2.12-3.1.3.jar:/usr/lib/spark/jars/generex-1.0.2.jar:/usr/lib/spark/jars/json4s-core_2.12-3.7.0-M5.jar:/usr/lib/spark/jars/antlr4-runtime-4.8-1.jar:/usr/lib/spark/jars/kerb-util-1.0.1.jar:/usr/lib/spark/jars/objenesis-2.6.jar:/usr/lib/spark/jars/metrics-jvm-4.1.1.jar:/usr/lib/spark/jars/opentracing-api-0.33.0.jar:/usr/lib/spark/jars/checker-qual-2.10.0.jar:/usr/lib/spark/jars/jdom2-2.0.6.jar:/usr/lib/spark/jars/hadoop-hdfs-client-3.2.4.jar:/usr/lib/spark/jars/netlib-native_ref-linux-x86_64-1.1-natives.jar:/usr/lib/spark/jars/jackson-core-asl-1.9.13.jar:/usr/lib/spark/jars/jersey-common-2.30.jar:/usr/lib/spark/jars/hadoop-mapreduce-client-core-3.2.4.jar:/usr/lib/spark/jars/azure-keyvault-core-1.0.0.jar:/usr/lib/spark/jars/javolution-5.5.1.jar:/usr/lib/spark/jars/json4s-ast_2.12-3.7.0-M5.jar:/usr/lib/spark/jars/hadoop-mapreduce-client-common-3.2.4.jar:/usr/lib/spark/jars/commons-dbcp-1.4.jar:/usr/lib/spark/jars/bcpkix-jdk15on-1.60.jar:/usr/lib/spark/jars/jcip-annotations-1.0-1.jar:/usr/lib/spark/jars/hive-exec-2.3.8-core.jar:/usr/lib/spark/jars/woodstox-core-5.4.0.jar:/usr/lib/spark/jars/kerb-admin-1.0.1.jar:/usr/lib/spark/jars/zstd-jni-1.4.8-1.jar:/usr/lib/spark/jars/HikariCP-2.5.1.jar:/usr/lib/spark/jars/hbase-spark-1.0.1-SNAPSHOT.jar:/usr/lib/spark/jars/spark-sql_2.12-3.1.3.jar:/usr/lib/spark/jars/dnsjava-2.1.7.jar:/usr/lib/spark/jars/okio-1.14.0.jar:/usr/lib/spark/jars/cats-kernel_2.12-2.0.0-M4.jar:/usr/lib/spark/jars/spark-mllib_2.12-3.1.3.jar:/usr/lib/spark/jars/jdo-api-3.0.1.jar:/usr/lib/spark/jars/jakarta.inject-2.6.1.jar:/usr/lib/spark/jars/metrics-core-4.1.1.jar:/usr/lib/spark/jars/hadoop-openstack-3.2.4.jar:/usr/lib/spark/jars/jackson-module-paranamer-2.10.5.jar:/usr/lib/spark/jars/dataproc-spark-plugins-1.0.0.jar:/usr/lib/spark/jars/commons-text-1.10.0.jar:/usr/lib/spark/jars/hive-metastore-2.3.8.jar:/usr/lib/spark/jars/jakarta.annotation-api-1.3.5.jar:/usr/lib/spark/jars/kubernetes-model-policy-4.12.0.jar:/usr/lib/spark/jars/jackson-databind-2.10.5.jar:/usr/lib/spark/jars/re2j-1.1.jar:/usr/lib/spark/jars/commons-lang3-3.10.jar:/usr/lib/spark/jars/spire-util_2.12-0.17.0-M1.jar:/usr/lib/spark/jars/commons-compress-1.21.jar:/usr/lib/spark/jars/jakarta.activation-api-1.2.1.jar:/usr/lib/spark/jars/arrow-memory-core-2.0.0.jar:/usr/lib/spark/jars/spark-repl_2.12-3.1.3.jar:/usr/lib/spark/jars/protobuf-java-2.5.0.jar:/usr/lib/spark/jars/commons-logging-1.1.3.jar:/usr/lib/spark/jars/netlib-native_ref-linux-armhf-1.1-natives.jar:/usr/lib/spark/jars/jackson-dataformat-cbor-2.10.5.jar:/usr/lib/spark/jars/macro-compat_2.12-1.1.1.jar:/usr/lib/spark/jars/hbase-spark-protocol-shaded-1.0.1-SNAPSHOT.jar:/usr/lib/spark/jars/kryo-shaded-4.0.2.jar:/usr/lib/spark/jars/kubernetes-model-common-4.12.0.jar:/usr/lib/spark/jars/arpack_combined_all-0.1.jar:/usr/lib/spark/jars/commons-net-3.1.jar:/usr/lib/spark/jars/hadoop-aws-3.2.4.jar:/usr/lib/spark/jars/commons-lang-2.6.jar:/usr/lib/spark/jars/spark-streaming_2.12-3.1.3.jar:/usr/lib/spark/jars/jackson-module-scala_2.12-2.10.5.jar:/usr/lib/spark/jars/super-csv-2.2.0.jar:/usr/lib/spark/jars/kerb-common-1.0.1.jar:/usr/lib/spark/jars/netlib-native_system-linux-i686-1.1-natives.jar:/usr/lib/spark/jars/kubernetes-model-discovery-4.12.0.jar:/usr/lib/spark/jars/netlib-native_ref-osx-x86_64-1.1-natives.jar:/usr/lib/spark/jars/machinist_2.12-0.6.8.jar:/usr/lib/spark/jars/zookeeper-3.4.14.jar:/usr/lib/spark/jars/univocity-parsers-2.9.1.jar:/usr/lib/spark/jars/kerby-pkix-1.0.1.jar:/usr/lib/spark/jars/jakarta.validation-api-2.0.2.jar:/usr/lib/spark/jars/jersey-media-jaxb-2.30.jar:/usr/lib/spark/jars/core-1.1.2.jar:/usr/lib/spark/jars/hive-shims-scheduler-2.3.8.jar:/usr/lib/spark/jars/jsr305-3.0.0.jar:/usr/lib/spark/jars/kubernetes-model-rbac-4.12.0.jar:/usr/lib/spark/jars/json4s-scalap_2.12-3.7.0-M5.jar:/usr/lib/spark/jars/oro-2.0.8.jar:/usr/lib/spark/jars/snakeyaml-2.0.jar:/usr/lib/spark/jars/logging-interceptor-3.12.12.jar:/usr/lib/spark/jars/hadoop-yarn-server-web-proxy-3.2.4.jar:/usr/lib/spark/jars/hadoop-client-3.2.4.jar:/usr/lib/spark/jars/pyrolite-4.30.jar:/usr/lib/spark/jars/jpam-1.1.jar:/usr/lib/spark/jars/netlib-native_system-win-i686-1.1-natives.jar:/usr/lib/spark/jars/kubernetes-model-settings-4.12.0.jar:/usr/lib/spark/jars/slf4j-reload4j-1.7.36.jar:/usr/lib/spark/jars/kubernetes-model-admissionregistration-4.12.0.jar:/usr/lib/spark/jars/spark-kvstore_2.12-3.1.3.jar:/usr/lib/spark/jars/kerby-xdr-1.0.1.jar:/usr/lib/spark/jars/automaton-1.11-8.jar:/usr/lib/spark/jars/guice-servlet-4.0.jar:/usr/lib/spark/jars/kerb-server-1.0.1.jar:/usr/lib/spark/jars/kubernetes-model-batch-4.12.0.jar:/usr/lib/spark/jars/commons-pool-1.5.4.jar:/usr/lib/spark/jars/jersey-server-2.30.jar:/usr/lib/spark/jars/libthrift-0.12.0.jar:/usr/lib/spark/jars/failureaccess-1.0.1.jar:/usr/lib/spark/jars/tink-1.6.0.jar:/usr/lib/spark/jars/netlib-native_ref-win-i686-1.1-natives.jar:/usr/lib/spark/jars/jline-2.14.6.jar:/usr/lib/spark/jars/spark-tags_2.12-3.1.3.jar:/usr/lib/spark/jars/metrics-graphite-4.1.1.jar:/usr/lib/spark/jars/reload4j-1.2.22.jar:/usr/lib/spark/jars/paranamer-2.8.jar:/usr/lib/spark/jars/parquet-common-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/kerby-util-1.0.1.jar:/usr/lib/spark/jars/htrace-core4-4.1.0-incubating.jar:/usr/lib/spark/jars/javax.annotation-api-1.3.2.jar:/usr/lib/spark/jars/hadoop-common-3.2.4.jar:/usr/lib/spark/jars/native_system-java-1.1.jar:/usr/lib/spark/jars/okhttp-2.7.5.jar:/usr/lib/spark/jars/scala-collection-compat_2.12-2.1.1.jar:/usr/lib/spark/jars/netlib-native_system-osx-x86_64-1.1-natives.jar:/usr/lib/spark/jars/stream-2.9.6.jar:/usr/lib/spark/jars/kerb-core-1.0.1.jar:/usr/lib/spark/jars/spire-platform_2.12-0.17.0-M1.jar:/usr/lib/spark/jars/metrics-jmx-4.1.1.jar:/usr/lib/spark/jars/json-smart-2.4.9.jar:/usr/lib/spark/jars/wildfly-openssl-1.0.7.Final.jar:/usr/lib/spark/jars/hadoop-azure-3.2.4.jar:/usr/lib/spark/jars/commons-httpclient-3.1.jar:/usr/lib/spark/jars/spark-core_2.12-3.1.3.jar:/usr/lib/spark/jars/hive-shims-common-2.3.8.jar:/usr/lib/spark/jars/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/spark/jars/commons-io-2.5.jar:/usr/lib/spark/jars/commons-crypto-1.1.0.jar:/usr/lib/spark/jars/curator-framework-2.13.0.jar:/usr/lib/spark/jars/json4s-jackson_2.12-3.7.0-M5.jar:/usr/lib/spark/jars/javassist-3.25.0-GA.jar:/usr/lib/spark/jars/javax.jdo-3.2.0-m3.jar:/usr/lib/spark/jars/hive-vector-code-gen-2.3.8.jar:/usr/lib/spark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/usr/lib/spark/jars/jaxb-api-2.2.11.jar:/usr/lib/spark/jars/shapeless_2.12-2.3.3.jar:/usr/lib/spark/jars/kerby-config-1.0.1.jar:/usr/lib/spark/jars/aliyun-java-sdk-core-4.5.10.jar:/usr/lib/spark/jars/kubernetes-model-networking-4.12.0.jar:/usr/lib/spark/jars/hbase-spark-protocol-1.0.1-SNAPSHOT.jar:/usr/lib/spark/jars/jackson-mapper-asl-1.9.13.jar:/usr/lib/spark/jars/jersey-container-servlet-2.30.jar:/usr/lib/spark/jars/metrics-json-4.1.1.jar:/usr/lib/spark/jars/error_prone_annotations-2.3.4.jar:/usr/lib/spark/jars/spark-hive-thriftserver_2.12-3.1.3.jar:/usr/lib/spark/jars/hive-jdbc-2.3.8.jar:/usr/lib/spark/jars/jakarta.ws.rs-api-2.1.6.jar:/usr/lib/spark/jars/parquet-format-structures-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/jackson-datatype-jsr310-2.11.2.jar:/usr/lib/spark/jars/spark-catalyst_2.12-3.1.3.jar:/usr/lib/spark/jars/jersey-hk2-2.30.jar:/usr/lib/spark/jars/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/spark/jars/kerb-client-1.0.1.jar:/usr/lib/spark/jars/json-1.8.jar:/usr/lib/spark/jars/velocity-engine-core-2.2.jar:/usr/lib/spark/jars/stax-api-1.0.1.jar:/usr/lib/spark/jars/spark-network-common_2.12-3.1.3.jar:/usr/lib/spark/jars/joda-time-2.10.5.jar:/usr/lib/spark/jars/arrow-vector-2.0.0.jar:/usr/lib/spark/jars/lz4-java-1.7.1.jar:/usr/lib/spark/jars/hadoop-aliyun-3.2.4.jar:/usr/lib/spark/jars/commons-math3-3.4.1.jar:/usr/lib/spark/jars/hive-common-2.3.8.jar:/usr/lib/spark/jars/parquet-column-1.11.2-dataproc-1.0.2.jar:/usr/lib/spark/jars/hadoop-annotations-3.2.4.jar:/usr/lib/spark/jars/jcl-over-slf4j-1.7.36.jar:/usr/lib/spark/jars/leveldbjni-all-1.8.jar:/usr/lib/spark/jars/kubernetes-model-metrics-4.12.0.jar:/usr/lib/spark/jars/ehcache-3.3.1.jar:/usr/lib/spark/jars/avro-ipc-1.9.2.jar:/usr/lib/spark/jars/spark-kubernetes_2.12-3.1.3.jar:/usr/lib/spark/jars/jackson-core-2.10.5.jar:/usr/lib/spark/jars/opentracing-noop-0.33.0.jar:/usr/lib/spark/jars/opentracing-util-0.33.0.jar:/etc/hadoop/conf/:/etc/hive/conf/:/usr/local/share/google/dataproc/lib/spark-metrics-listener.jar:/usr/local/share/google/dataproc/lib/ranger_gcs_plugin_client.jar:/usr/local/share/google/dataproc/lib/bigquery-metastore-client-0.0.20251014-jar-with-dependencies.jar:/usr/local/share/google/dataproc/lib/spark-metrics-listener-dataproc-2.0-1.15.0.jar:/usr/local/share/google/dataproc/lib/gcs-connector-hadoop3-2.2.29.jar:/usr/local/share/google/dataproc/lib/gcs-connector.jar:/usr/share/java/mysql.jar\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.library.path=:/usr/lib/hadoop/lib/native:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.compiler=<NA>\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.name=Linux\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.arch=amd64\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.version=5.10.0-0.deb10.16-cloud-amd64\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.name=root\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.home=/root\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.dir=/tmp/9e330051be42431ba95b7060b09f71c8\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=10.128.0.2:9983 sessionTimeout=30000 watcher=org.apache.solr.common.cloud.SolrZkClient$ProcessWatchWithExecutor@13ab7bb\n",
      "25/11/25 02:40:24 INFO org.apache.solr.common.cloud.ConnectionManager: Waiting for client to connect to ZooKeeper\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server solr-instance.us-central1-a.c.family-tree-469815.internal/10.128.0.2:9983. Will not attempt to authenticate using SASL (unknown error)\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to solr-instance.us-central1-a.c.family-tree-469815.internal/10.128.0.2:9983, initiating session\n",
      "25/11/25 02:40:24 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server solr-instance.us-central1-a.c.family-tree-469815.internal/10.128.0.2:9983, sessionid = 0x10000064e510002, negotiated timeout = 30000\n",
      "25/11/25 02:40:24 INFO org.apache.solr.common.cloud.ConnectionManager: zkClient has connected\n",
      "25/11/25 02:40:24 INFO org.apache.solr.common.cloud.ConnectionManager: Client is connected to ZooKeeper\n",
      "25/11/25 02:40:24 INFO org.apache.solr.common.cloud.ZkStateReader: Updated live nodes from ZooKeeper... (0) -> (1)\n",
      "25/11/25 02:40:24 INFO org.apache.solr.client.solrj.impl.ZkClientClusterStateProvider: Cluster at 10.128.0.2:9983 ready\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> author, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> string\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> category, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> string\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> created_at, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> string\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> description, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> string\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> in_stock, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> boolean\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> price, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> pdouble\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: adding new field: name -> title, indexed -> true, multiValued -> false, docValues -> true, stored -> true, type -> string\n",
      "25/11/25 02:40:25 INFO com.lucidworks.spark.SolrRelation$: Sending request to Solr schema API to add 7 fields.\n"
     ]
    }
   ],
   "source": [
    "# Check if indexing is already complete\n",
    "import requests\n",
    "\n",
    "def check_gcp_indexing_complete():\n",
    "    \"\"\"Check if data is already indexed in GCP Solr\"\"\"\n",
    "    try:\n",
    "        # Get local document count\n",
    "        with open(\"data/dummy_data.json\") as f:\n",
    "            local_count = sum(1 for _ in f)\n",
    "        \n",
    "        # Get Solr document count\n",
    "        response = requests.get(\n",
    "            f\"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=*:*&rows=0\", \n",
    "            timeout=10\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            solr_count = response.json()['response']['numFound']\n",
    "            \n",
    "            if local_count == solr_count and solr_count > 0:\n",
    "                # Verify sample document exists\n",
    "                with open(\"data/dummy_data.json\") as f:\n",
    "                    first_doc = json.loads(f.readline())\n",
    "                    doc_id = first_doc['id']\n",
    "                \n",
    "                check_response = requests.get(\n",
    "                    f\"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=id:{doc_id}&rows=1\",\n",
    "                    timeout=10\n",
    "                )\n",
    "                if check_response.status_code == 200:\n",
    "                    match_count = check_response.json()['response']['numFound']\n",
    "                    if match_count > 0:\n",
    "                        return True, solr_count\n",
    "        return False, 0\n",
    "    except Exception as e:\n",
    "        print(f\"Check failed: {e}\")\n",
    "        return False, 0\n",
    "\n",
    "already_indexed, doc_count = check_gcp_indexing_complete()\n",
    "\n",
    "if already_indexed:\n",
    "    print(f\"⏭️  Skipping indexing: {doc_count} documents already indexed and verified in GCP Solr\")\n",
    "else:\n",
    "    # Get Solr Internal IP for ZooKeeper connection (required by spark-solr)\n",
    "    ip_result = subprocess.run(\n",
    "        f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(networkInterfaces[0].networkIP)'\",\n",
    "        shell=True, capture_output=True, text=True\n",
    "    )\n",
    "    SOLR_INTERNAL_IP = ip_result.stdout.strip()\n",
    "    print(f\"✓ Solr Internal IP: {SOLR_INTERNAL_IP}\")\n",
    "\n",
    "    # Always upload the latest version of the job to ensure fixes are applied\n",
    "    # Modify the Spark job to use GCS paths and remote Solr\n",
    "    gcp_spark_job = f\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\\\n",
    "        .appName(\"SolrIndexer-GCP\") \\\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Read JSON data from GCS\n",
    "    input_file = \"gs://{BUCKET_NAME}/data/dummy_data.json\"\n",
    "    print(f\"Reading data from {{input_file}}\")\n",
    "    \n",
    "    df = spark.read.json(input_file)\n",
    "    \n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Solr configuration\n",
    "    # Use Internal IP for ZooKeeper (accessible within VPC)\n",
    "    zk_host = \"{SOLR_INTERNAL_IP}:9983\"\n",
    "    collection = \"dummy_data\"\n",
    "    \n",
    "    print(f\"Indexing to Solr collection '{{collection}}' via ZK '{{zk_host}}'...\")\n",
    "    \n",
    "    # Write to Solr using ZK (standard method)\n",
    "    df.write.format(\"solr\") \\\\\n",
    "        .option(\"zkhost\", zk_host) \\\\\n",
    "        .option(\"collection\", collection) \\\\\n",
    "        .option(\"gen_uniq_key\", \"true\") \\\\\n",
    "        .option(\"commit_within\", \"1000\") \\\\\n",
    "        .mode(\"overwrite\") \\\\\n",
    "        .save()\n",
    "        \n",
    "    print(\"Indexing complete.\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "    # Save and upload job\n",
    "    os.makedirs(\"spark_job\", exist_ok=True)\n",
    "    with open(\"spark_job/index_to_solr_gcp.py\", \"w\") as f:\n",
    "        f.write(gcp_spark_job)\n",
    "    \n",
    "    # Upload using gsutil\n",
    "    upload_job_cmd = f\"gsutil cp spark_job/index_to_solr_gcp.py gs://{BUCKET_NAME}/jobs/\"\n",
    "    subprocess.run(upload_job_cmd, shell=True, check=True)\n",
    "    print(f\"✓ Uploaded job to gs://{BUCKET_NAME}/jobs/index_to_solr_gcp.py\")\n",
    "    \n",
    "    # Submit job to Dataproc using pre-downloaded JAR (much faster)\n",
    "    submit_job_cmd = f\"\"\"\n",
    "gcloud dataproc jobs submit pyspark \\\\\n",
    "    gs://{BUCKET_NAME}/jobs/index_to_solr_gcp.py \\\\\n",
    "    --cluster={CLUSTER_NAME} \\\\\n",
    "    --region={REGION} \\\\\n",
    "    --jars=gs://{BUCKET_NAME}/jars/spark-solr-4.0.0-shaded.jar\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Submitting Spark job to Dataproc...\")\n",
    "    !{submit_job_cmd}\n",
    "    print(\"✓ Job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad68bc2",
   "metadata": {},
   "source": [
    "## 7. Verify Indexing\n",
    "\n",
    "Query the GCP-hosted Solr instance to verify data was indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b85eef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Indexed 0 documents in Solr\n",
      "\n",
      "Sample documents:\n",
      "{\n",
      "    \"responseHeader\": {\n",
      "        \"zkConnected\": true,\n",
      "        \"status\": 0,\n",
      "        \"QTime\": 1,\n",
      "        \"params\": {\n",
      "            \"q\": \"*:*\",\n",
      "            \"rows\": \"3\"\n",
      "        }\n",
      "    },\n",
      "    \"response\": {\n",
      "        \"numFound\": 0,\n",
      "        \"start\": 0,\n",
      "        \"numFoundExact\": true,\n",
      "        \"docs\": []\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"responseHeader\": {\n",
      "        \"zkConnected\": true,\n",
      "        \"status\": 0,\n",
      "        \"QTime\": 1,\n",
      "        \"params\": {\n",
      "            \"q\": \"*:*\",\n",
      "            \"rows\": \"3\"\n",
      "        }\n",
      "    },\n",
      "    \"response\": {\n",
      "        \"numFound\": 0,\n",
      "        \"start\": 0,\n",
      "        \"numFoundExact\": true,\n",
      "        \"docs\": []\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Query Solr for document count\n",
    "try:\n",
    "    response = requests.get(f\"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=*:*&rows=0\")\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        num_docs = result['response']['numFound']\n",
    "        print(f\"✓ Indexed {num_docs} documents in Solr\")\n",
    "    else:\n",
    "        print(\"✗ Failed to query Solr\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Query failed: {e}\")\n",
    "\n",
    "# Show sample documents\n",
    "print(\"\\nSample documents:\")\n",
    "!curl -s \"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=*:*&rows=3\" | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12985d18",
   "metadata": {},
   "source": [
    "## 8. Cleanup Resources\n",
    "\n",
    "**Important:** Delete GCP resources to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74caeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up all GCP resources\n",
    "\n",
    "# # Delete Dataproc cluster\n",
    "# !gcloud dataproc clusters delete {CLUSTER_NAME} --region={REGION} --quiet\n",
    "# print(\"✓ Deleted Dataproc cluster\")\n",
    "\n",
    "# # Delete Solr VM\n",
    "# !gcloud compute instances delete {SOLR_VM_NAME} --zone={ZONE} --quiet\n",
    "# print(\"✓ Deleted Solr VM\")\n",
    "\n",
    "# # Delete firewall rule\n",
    "# !gcloud compute firewall-rules delete allow-solr --quiet\n",
    "# print(\"✓ Deleted firewall rule\")\n",
    "\n",
    "# # Delete GCS bucket\n",
    "# !gsutil -m rm -r gs://{BUCKET_NAME}\n",
    "# print(\"✓ Deleted GCS bucket\")\n",
    "\n",
    "print(\"To clean up, uncomment the commands above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3d444",
   "metadata": {},
   "source": [
    "## Cost Estimation\n",
    "\n",
    "- **Dataproc Cluster**: ~$0.50-1.00/hour (2 workers + 1 master)\n",
    "- **Solr VM**: ~$0.10-0.20/hour (n1-standard-2)\n",
    "- **Storage**: ~$0.02/GB/month\n",
    "- **Network Egress**: Variable\n",
    "\n",
    "**Remember to delete resources when not in use!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
