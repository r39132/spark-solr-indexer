{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b21f644",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Google Cloud SDK (`gcloud`) installed\n",
    "- GCP project with billing enabled\n",
    "- Dataproc API enabled\n",
    "- Compute Engine API enabled (for Solr VM)\n",
    "- Appropriate IAM permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3427ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "from google.cloud import storage\n",
    "from google.cloud import dataproc_v1\n",
    "\n",
    "# Ensure we are in the project root\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a4c37",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your GCP project details and cluster configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ea799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Configuration\n",
    "PROJECT_ID = \"your-project-id\"  # TODO: Set your GCP project ID\n",
    "REGION = \"us-central1\"\n",
    "ZONE = \"us-central1-a\"\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-spark-solr-data\"  # GCS bucket for data\n",
    "\n",
    "# Dataproc Configuration\n",
    "CLUSTER_NAME = \"spark-solr-cluster\"\n",
    "DATAPROC_MASTER_TYPE = \"n1-standard-4\"\n",
    "DATAPROC_WORKER_TYPE = \"n1-standard-4\"\n",
    "DATAPROC_WORKER_COUNT = 2\n",
    "\n",
    "# Solr Configuration (VM-based)\n",
    "SOLR_VM_NAME = \"solr-instance\"\n",
    "SOLR_VM_TYPE = \"n1-standard-2\"\n",
    "SOLR_EXTERNAL_IP = None  # Will be set after VM creation\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Bucket: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632356b",
   "metadata": {},
   "source": [
    "## 1. Authenticate with GCP\n",
    "\n",
    "Login to Google Cloud Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501dd782",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb59ba",
   "metadata": {},
   "source": [
    "## 2. Create GCS Bucket\n",
    "\n",
    "Create a Cloud Storage bucket to store data and scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "try:\n",
    "    bucket = storage_client.create_bucket(BUCKET_NAME, location=REGION)\n",
    "    print(f\"✓ Created bucket: {BUCKET_NAME}\")\n",
    "except Exception as e:\n",
    "    if \"409\" in str(e):  # Bucket already exists\n",
    "        bucket = storage_client.bucket(BUCKET_NAME)\n",
    "        print(f\"✓ Using existing bucket: {BUCKET_NAME}\")\n",
    "    else:\n",
    "        print(f\"✗ Failed to create bucket: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3210c20",
   "metadata": {},
   "source": [
    "## 3. Generate and Upload Data\n",
    "\n",
    "Generate dummy data locally and upload to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db368df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data already exists locally\n",
    "local_data_exists = False\n",
    "if os.path.exists(\"data/dummy_data.json\"):\n",
    "    with open(\"data/dummy_data.json\") as f:\n",
    "        lines = f.readlines()\n",
    "    if len(lines) > 0:\n",
    "        print(f\"⏭️  Using existing local data: {len(lines)} records\")\n",
    "        local_data_exists = True\n",
    "\n",
    "# Generate data if needed\n",
    "if not local_data_exists:\n",
    "    print(\"Generating data...\")\n",
    "    !python3 data_gen/generate_data.py\n",
    "    with open(\"data/dummy_data.json\") as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"✓ Generated {len(lines)} records locally\")\n",
    "\n",
    "# Check if data already exists in GCS\n",
    "blob = bucket.blob(\"data/dummy_data.json\")\n",
    "if blob.exists():\n",
    "    print(f\"⏭️  Data already exists in GCS: gs://{BUCKET_NAME}/data/dummy_data.json\")\n",
    "else:\n",
    "    # Upload to GCS\n",
    "    blob.upload_from_filename(\"data/dummy_data.json\")\n",
    "    print(f\"✓ Uploaded to gs://{BUCKET_NAME}/data/dummy_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19986ec9",
   "metadata": {},
   "source": [
    "## 4. Create Solr VM on GCE\n",
    "\n",
    "Launch a Compute Engine VM and install Solr Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c77bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Solr VM already exists\n",
    "vm_exists_cmd = f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(name)' 2>/dev/null\"\n",
    "result = subprocess.run(vm_exists_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.stdout.strip() == SOLR_VM_NAME:\n",
    "    print(f\"⏭️  Solr VM '{SOLR_VM_NAME}' already exists\")\n",
    "    # Get existing IP\n",
    "    ip_result = subprocess.run(\n",
    "        f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\",\n",
    "        shell=True, capture_output=True, text=True\n",
    "    )\n",
    "    SOLR_EXTERNAL_IP = ip_result.stdout.strip()\n",
    "    print(f\"  Solr URL: http://{SOLR_EXTERNAL_IP}:8983\")\n",
    "else:\n",
    "    # Create Solr VM with startup script\n",
    "    startup_script = \"\"\"#!/bin/bash\n",
    "apt-get update\n",
    "apt-get install -y openjdk-11-jdk wget\n",
    "\n",
    "# Download and setup Solr\n",
    "cd /opt\n",
    "wget https://archive.apache.org/dist/lucene/solr/8.11.3/solr-8.11.3.tgz\n",
    "tar xzf solr-8.11.3.tgz\n",
    "cd solr-8.11.3\n",
    "\n",
    "# Start Solr in cloud mode\n",
    "bin/solr start -c -m 2g\n",
    "\n",
    "# Create collection\n",
    "bin/solr create -c dummy_data -s 1 -rf 1\n",
    "\n",
    "echo \"Solr started on port 8983\"\n",
    "\"\"\"\n",
    "\n",
    "    # Create VM\n",
    "    create_vm_cmd = f\"\"\"\n",
    "gcloud compute instances create {SOLR_VM_NAME} \\\\\n",
    "    --project={PROJECT_ID} \\\\\n",
    "    --zone={ZONE} \\\\\n",
    "    --machine-type={SOLR_VM_TYPE} \\\\\n",
    "    --image-family=debian-11 \\\\\n",
    "    --image-project=debian-cloud \\\\\n",
    "    --boot-disk-size=50GB \\\\\n",
    "    --tags=solr-server \\\\\n",
    "    --metadata=startup-script='{startup_script}'\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Creating Solr VM...\")\n",
    "    !{create_vm_cmd}\n",
    "\n",
    "    # Create firewall rule for Solr\n",
    "    !gcloud compute firewall-rules create allow-solr \\\n",
    "        --project={PROJECT_ID} \\\n",
    "        --allow=tcp:8983 \\\n",
    "        --target-tags=solr-server \\\n",
    "        --description=\"Allow Solr traffic\" \\\n",
    "        2>/dev/null || echo \"Firewall rule already exists\"\n",
    "\n",
    "    # Wait for VM to be ready\n",
    "    time.sleep(60)\n",
    "\n",
    "    # Get external IP\n",
    "    result = subprocess.run(\n",
    "        f\"gcloud compute instances describe {SOLR_VM_NAME} --zone={ZONE} --format='get(networkInterfaces[0].accessConfigs[0].natIP)'\",\n",
    "        shell=True, capture_output=True, text=True\n",
    "    )\n",
    "    SOLR_EXTERNAL_IP = result.stdout.strip()\n",
    "    print(f\"✓ Solr VM created with IP: {SOLR_EXTERNAL_IP}\")\n",
    "    print(f\"  Solr URL: http://{SOLR_EXTERNAL_IP}:8983\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1e60f",
   "metadata": {},
   "source": [
    "## 5. Create Dataproc Cluster\n",
    "\n",
    "Launch a Dataproc cluster for running Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce303216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Dataproc cluster already exists\n",
    "cluster_exists_cmd = f\"gcloud dataproc clusters describe {CLUSTER_NAME} --region={REGION} --format='get(clusterName)' 2>/dev/null\"\n",
    "result = subprocess.run(cluster_exists_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.stdout.strip() == CLUSTER_NAME:\n",
    "    print(f\"⏭️  Dataproc cluster '{CLUSTER_NAME}' already exists\")\n",
    "else:\n",
    "    # Create Dataproc cluster\n",
    "    create_cluster_cmd = f\"\"\"\n",
    "gcloud dataproc clusters create {CLUSTER_NAME} \\\\\n",
    "    --project={PROJECT_ID} \\\\\n",
    "    --region={REGION} \\\\\n",
    "    --master-machine-type={DATAPROC_MASTER_TYPE} \\\\\n",
    "    --worker-machine-type={DATAPROC_WORKER_TYPE} \\\\\n",
    "    --num-workers={DATAPROC_WORKER_COUNT} \\\\\n",
    "    --image-version=2.1-debian11 \\\\\n",
    "    --enable-component-gateway \\\\\n",
    "    --optional-components=JUPYTER \\\\\n",
    "    --max-idle=3600s\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Creating Dataproc cluster (this may take 3-5 minutes)...\")\n",
    "    !{create_cluster_cmd}\n",
    "    print(f\"✓ Dataproc cluster '{CLUSTER_NAME}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337bb53",
   "metadata": {},
   "source": [
    "## 6. Index Data with Dataproc\n",
    "\n",
    "Submit the Spark job to index data into GCP Solr. This step checks if indexing is already complete and skips if verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbecf5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if indexing is already complete\n",
    "import requests\n",
    "\n",
    "def check_gcp_indexing_complete():\n",
    "    \"\"\"Check if data is already indexed in GCP Solr\"\"\"\n",
    "    try:\n",
    "        # Get local document count\n",
    "        with open(\"data/dummy_data.json\") as f:\n",
    "            local_count = sum(1 for _ in f)\n",
    "        \n",
    "        # Get Solr document count\n",
    "        response = requests.get(\n",
    "            f\"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=*:*&rows=0\", \n",
    "            timeout=10\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            solr_count = response.json()['response']['numFound']\n",
    "            \n",
    "            if local_count == solr_count and solr_count > 0:\n",
    "                # Verify sample document exists\n",
    "                with open(\"data/dummy_data.json\") as f:\n",
    "                    first_doc = json.loads(f.readline())\n",
    "                    doc_id = first_doc['id']\n",
    "                \n",
    "                check_response = requests.get(\n",
    "                    f\"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=id:{doc_id}&rows=1\",\n",
    "                    timeout=10\n",
    "                )\n",
    "                if check_response.status_code == 200:\n",
    "                    match_count = check_response.json()['response']['numFound']\n",
    "                    if match_count > 0:\n",
    "                        return True, solr_count\n",
    "        return False, 0\n",
    "    except Exception as e:\n",
    "        print(f\"Check failed: {e}\")\n",
    "        return False, 0\n",
    "\n",
    "already_indexed, doc_count = check_gcp_indexing_complete()\n",
    "\n",
    "if already_indexed:\n",
    "    print(f\"⏭️  Skipping indexing: {doc_count} documents already indexed and verified in GCP Solr\")\n",
    "else:\n",
    "    # Upload job to GCS if not already there\n",
    "    blob = bucket.blob(\"jobs/index_to_solr_gcp.py\")\n",
    "    if not blob.exists():\n",
    "        # Modify the Spark job to use GCS paths and remote Solr\n",
    "        gcp_spark_job = f\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\\\n",
    "        .appName(\"SolrIndexer-GCP\") \\\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Read JSON data from GCS\n",
    "    input_file = \"gs://{BUCKET_NAME}/data/dummy_data.json\"\n",
    "    print(f\"Reading data from {{input_file}}\")\n",
    "    \n",
    "    df = spark.read.json(input_file)\n",
    "    \n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Solr configuration - using external Solr VM\n",
    "    # Note: In production, use internal IP and VPC peering\n",
    "    zk_host = \"{SOLR_EXTERNAL_IP}:9983\"\n",
    "    collection = \"dummy_data\"\n",
    "    \n",
    "    print(f\"Indexing to Solr collection '{{collection}}' at ZK '{{zk_host}}'...\")\n",
    "    \n",
    "    # Write to Solr\n",
    "    df.write.format(\"solr\") \\\\\n",
    "        .option(\"zkhost\", zk_host) \\\\\n",
    "        .option(\"collection\", collection) \\\\\n",
    "        .option(\"gen_uniq_key\", \"true\") \\\\\n",
    "        .option(\"commit_within\", \"1000\") \\\\\n",
    "        .mode(\"overwrite\") \\\\\n",
    "        .save()\n",
    "        \n",
    "    print(\"Indexing complete.\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "        # Save and upload job\n",
    "        with open(\"spark_job/index_to_solr_gcp.py\", \"w\") as f:\n",
    "            f.write(gcp_spark_job)\n",
    "        blob.upload_from_filename(\"spark_job/index_to_solr_gcp.py\")\n",
    "        print(f\"✓ Uploaded job to gs://{BUCKET_NAME}/jobs/index_to_solr_gcp.py\")\n",
    "    \n",
    "    # Submit job to Dataproc\n",
    "    submit_job_cmd = f\"\"\"\n",
    "gcloud dataproc jobs submit pyspark \\\\\n",
    "    gs://{BUCKET_NAME}/jobs/index_to_solr_gcp.py \\\\\n",
    "    --cluster={CLUSTER_NAME} \\\\\n",
    "    --region={REGION} \\\\\n",
    "    --jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar \\\\\n",
    "    --properties=spark.jars.packages=com.lucidworks.spark:spark-solr:4.0.0\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Submitting Spark job to Dataproc...\")\n",
    "    !{submit_job_cmd}\n",
    "    print(\"✓ Job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad68bc2",
   "metadata": {},
   "source": [
    "## 7. Verify Indexing\n",
    "\n",
    "Query the GCP-hosted Solr instance to verify data was indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85eef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Query Solr for document count\n",
    "try:\n",
    "    response = requests.get(f\"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=*:*&rows=0\")\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        num_docs = result['response']['numFound']\n",
    "        print(f\"✓ Indexed {num_docs} documents in Solr\")\n",
    "    else:\n",
    "        print(\"✗ Failed to query Solr\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Query failed: {e}\")\n",
    "\n",
    "# Show sample documents\n",
    "print(\"\\nSample documents:\")\n",
    "!curl -s \"http://{SOLR_EXTERNAL_IP}:8983/solr/dummy_data/select?q=*:*&rows=3\" | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12985d18",
   "metadata": {},
   "source": [
    "## 8. Cleanup Resources\n",
    "\n",
    "**Important:** Delete GCP resources to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74caeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up all GCP resources\n",
    "\n",
    "# # Delete Dataproc cluster\n",
    "# !gcloud dataproc clusters delete {CLUSTER_NAME} --region={REGION} --quiet\n",
    "# print(\"✓ Deleted Dataproc cluster\")\n",
    "\n",
    "# # Delete Solr VM\n",
    "# !gcloud compute instances delete {SOLR_VM_NAME} --zone={ZONE} --quiet\n",
    "# print(\"✓ Deleted Solr VM\")\n",
    "\n",
    "# # Delete firewall rule\n",
    "# !gcloud compute firewall-rules delete allow-solr --quiet\n",
    "# print(\"✓ Deleted firewall rule\")\n",
    "\n",
    "# # Delete GCS bucket\n",
    "# !gsutil -m rm -r gs://{BUCKET_NAME}\n",
    "# print(\"✓ Deleted GCS bucket\")\n",
    "\n",
    "print(\"To clean up, uncomment the commands above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3d444",
   "metadata": {},
   "source": [
    "## Cost Estimation\n",
    "\n",
    "- **Dataproc Cluster**: ~$0.50-1.00/hour (2 workers + 1 master)\n",
    "- **Solr VM**: ~$0.10-0.20/hour (n1-standard-2)\n",
    "- **Storage**: ~$0.02/GB/month\n",
    "- **Network Egress**: Variable\n",
    "\n",
    "**Remember to delete resources when not in use!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
